{"cells":[{"cell_type":"markdown","metadata":{"id":"Z3jnZ4fh0_Z0"},"source":["# VAE + KMeans + Boundary Clusters Detection\n","\n","## Цель\n","Целью настоящего исследования является разработка гибридного алгоритма кластеризации на основе комбинации Variational Autoencoder и метода K-Means с механизмом обнаружения промежуточных кластеров  предоставляющих дополнительную информацию о переходных кластерах между основными кластерами для практического применения.\n","\n","**Промежуточные кластеры** - это плотные кластеры между основными кластерами, которые пересекаются. Для бизнеса это дополнительная важная информация о пользователях, которых нельзя однозначно отнести к какой-либо группе\n","\n","\n","---\n","\n","## Научная новизна\n","\n","### 1: Интегральная boundary-метрика\n","Комбинирует:\n","- **Соотношение расстояний**: $r = \\frac{d_1}{d_2}$ (расстояния до двух ближайших центров)\n","- **Энтропию соседей**: $H = -\\sum_{i} p_i \\log p_i$ (разнообразие меток среди k соседей)\n","- **Комбинированная метрика**: $score = 0.9\\cdot r_{norm} + 0.1 \\cdot H_{norm}$\n","\n","### 2: Схема группировки boundary-точек\n","1. Группировка по парам ближайших основных кластеров $(c_i, c_j)$\n","2. DBSCAN при превышении лимита кластеров\n","\n","### 3: Boundary-метрики в оптимизации\n","Использование boundary-метрик как целевого критерия при оптимизации гиперпараметров.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jV__pjxnkoJn","cellView":"form"},"outputs":[],"source":["# @title\n","# Установка зависимостей и импорт библиотек\n","import sys\n","import subprocess\n","import os\n","\n","# Функция установки зависимостей из requirements.txt\n","def install_from_requirements():\n","    \"\"\"\n","    Устанавливает зависимости из requirements.txt\n","    \"\"\"\n","    current_dir = os.getcwd()\n","    requirements_path = os.path.join(current_dir, \"requirements.txt\")\n","\n","    if not os.path.exists(requirements_path):\n","        parent_dir = os.path.dirname(current_dir)\n","        requirements_path = os.path.join(parent_dir, \"requirements.txt\")\n","\n","        if not os.path.exists(requirements_path):\n","            return install_manually()\n","\n","    try:\n","        result = subprocess.run(\n","            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", requirements_path],\n","            capture_output=True,\n","            text=True\n","        )\n","\n","        if result.returncode == 0:\n","            return True\n","        else:\n","            return install_manually()\n","    except Exception:\n","        return install_manually()\n","\n","# Функция установки зависимостей вручную (fallback)\n","def install_manually():\n","    \"\"\"\n","    Устанавливает зависимости вручную (fallback)\n","    \"\"\"\n","    packages = [\n","        \"pandas>=1.5.0\",\n","        \"numpy>=1.21.0\",\n","        \"matplotlib>=3.5.0\",\n","        \"seaborn>=0.12.0\",\n","        \"scikit-learn>=1.0.0\",\n","        \"scipy>=1.9.0\",\n","        \"torch>=1.12.0\",\n","        \"torchvision>=0.13.0\",\n","        \"scikit-optimize>=0.9.0\",\n","        \"statsmodels>=0.13.0\",\n","        \"scikit-fuzzy>=0.4.2\",\n","        \"plotly>=5.0.0\"\n","    ]\n","\n","    for package in packages:\n","        try:\n","            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n","        except Exception:\n","            pass\n","\n","    return True\n","\n","# Проверяем наличие основных библиотек и устанавливаем при необходимости\n","try:\n","    import pandas\n","    import numpy\n","    import matplotlib\n","    import seaborn\n","    import sklearn\n","    import scipy\n","    import torch\n","except ImportError:\n","    install_from_requirements()\n","\n","# Устанавливаем зависимости (если нужно)\n","install_from_requirements()\n","\n","# Добавляем текущую директорию в путь для импорта imports.py\n","current_dir = os.getcwd()\n","if current_dir not in sys.path:\n","    sys.path.insert(0, current_dir)\n","\n","# Импортируем все из модуля imports\n","from imports import *\n"]},{"cell_type":"markdown","metadata":{"id":"GRhVDU2XkoJo"},"source":["## Общая методология подхода\n","\n","Предлагаемый алгоритм кластеризации состоит из трех основных этапов:\n","\n","### 1. Снижение размерности через VAE\n","\n","**Зачем VAE?**\n","- VAE учится находить скрытые структуры в данных\n","- Латентное пространство VAE лучше подходит для кластеризации, чем исходное пространство признаков\n","- VAE автоматически извлекает наиболее важные признаки и их комбинации\n","- Регуляризация через KL divergence обеспечивает гладкое и интерпретируемое латентное пространство\n","\n","### 2. Кластеризация на латентном пространстве\n","\n","**KMeans на латентных представлениях**:\n","- Работает эффективнее, чем на исходных данных\n","- Находит основные, хорошо разделенные кластеры\n","- Использует среднее значение латентного распределения (μ) для детерминированности\n","\n","### 3. Обнаружение промежуточных (boundary) кластеров\n","\n","**Научная новизна**: Выделение промежуточных кластеров между основными кластерами:\n","- Использует интегральную метрику (расстояния + энтропия)\n","- Группирует boundary точки в отдельные кластеры\n","- Предоставляет дополнительную информацию для бизнеса о \"неопределенных\" пользователях\n"]},{"cell_type":"markdown","metadata":{"id":"B5IEi1os0_Z3"},"source":["## Предобработка данных\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4945-Tib0_Z3","outputId":"1c98f616-1ae6-42ed-e31e-49a54f2f8d66","executionInfo":{"status":"ok","timestamp":1766063393840,"user_tz":-180,"elapsed":12,"user":{"displayName":"Ульяна","userId":"09654476276403892729"}},"cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Функция предобработки определена\n"]}],"source":["# @title\n","def preprocess_dataframe(df, exclude_columns=None):\n","    \"\"\"\n","    Универсальная функция предобработки DataFrame.\n","\n","    Автоматически определяет числовые и категориальные признаки и выполняет:\n","    - Стандартизацию числовых признаков\n","    - One-hot encoding категориальных признаков\n","    - MinMax нормализацию для VAE (диапазон [0, 1])\n","\n","    Параметры:\n","    -----------\n","    df : pd.DataFrame\n","        Входной DataFrame\n","    exclude_columns : list, optional\n","        Список колонок для исключения из обработки\n","\n","    Возвращает:\n","    -----------\n","    X : np.ndarray\n","        Предобработанные данные (shape: [n_samples, n_features])\n","    preprocessor : ColumnTransformer\n","        Обученный препроцессор для будущего использования\n","    feature_info : dict\n","        Информация о признаках (числовые, категориальные)\n","    \"\"\"\n","\n","    # Определяем колонки для исключения\n","    exclude = set()\n","    if exclude_columns:\n","        exclude.update(exclude_columns)\n","\n","    # Автоматическое определение типов признаков\n","    numeric_features = []\n","    categorical_features = []\n","\n","    for col in df.columns:\n","        if col in exclude:\n","            continue\n","\n","        # Числовые признаки: int или float типы\n","        if df[col].dtype in ['int64', 'int32', 'float64', 'float32']:\n","            # Проверяем, что это действительно числовой признак (не ID)\n","            # Если уникальных значений меньше 20% от размера - считаем категориальным\n","            # Для маленьких датасетов: если уникальных значений <= 10, считаем категориальным\n","            # Для больших: если < 20% уникальных значений и < 50 уникальных\n","            nunique = df[col].nunique()\n","            n_samples = len(df)\n","\n","            # Проверка на бинарные признаки\n","            unique_values = set(df[col].dropna().unique())\n","            is_binary = (nunique == 2 and unique_values.issubset({0, 1, True, False, 0.0, 1.0}))\n","\n","            if is_binary:\n","                # Бинарный признак - оставляем как числовой\n","                numeric_features.append(col)\n","            elif n_samples <= 20:\n","                # Маленький датасет: более строгая проверка\n","                if nunique <= 10 and nunique < n_samples:\n","                    categorical_features.append(col)\n","                else:\n","                    numeric_features.append(col)\n","            else:\n","                # Большой датасет: стандартная логика\n","                if nunique < n_samples * 0.2 and nunique < 50:\n","                    categorical_features.append(col)\n","                else:\n","                    numeric_features.append(col)\n","        else:\n","            # Все остальное - категориальное\n","            categorical_features.append(col)\n","\n","    print(f\"Найдено признаков:\")\n","    print(f\"   - Числовых: {len(numeric_features)}\")\n","    print(f\"   - Категориальных: {len(categorical_features)}\")\n","\n","    # Проверка: все ли колонки обработаны\n","    processed_cols = set(numeric_features + categorical_features)\n","    all_cols = set(df.columns) - exclude\n","    unprocessed = all_cols - processed_cols\n","    if unprocessed:\n","        print(f\"Следующие колонки не обработаны: {list(unprocessed)}\")\n","        print(\"   Они будут исключены из анализа!\")\n","\n","    if len(numeric_features) == 0 and len(categorical_features) == 0:\n","        raise ValueError(\"Не найдено признаков для обработки!\")\n","\n","    # Обрабатываем NaN в числовых признаках ДО нормализации\n","    # Заполняем NaN медианным значением для каждого признака (более устойчиво к выбросам, чем среднее)\n","    for col in numeric_features:\n","        if df[col].isna().any():\n","            n_missing = df[col].isna().sum()\n","            median_value = df[col].median()\n","            df[col] = df[col].fillna(median_value)\n","            print(f\"   Заполнено {n_missing} пропущенных значений в '{col}' медианным значением: {median_value:.4f}\")\n","\n","    # Создание препроцессора\n","    transformers = []\n","    n_categories_before = {}  # Инициализация для случая, когда нет категориальных признаков\n","\n","    # Используем только MinMaxScaler для числовых признаков\n","    # MinMaxScaler нормализует данные в диапазон [0, 1] для VAE\n","    if numeric_features:\n","        transformers.append((\"num\", MinMaxScaler(feature_range=(0, 1)), numeric_features))\n","\n","    if categorical_features:\n","        # Обрабатываем NaN в категориальных признаках ДО преобразования в строки\n","        # Это обеспечивает последовательную обработку пропущенных значений\n","        for col in categorical_features:\n","            # Заполняем NaN явным значением \"_MISSING_\" для создания отдельной категории\n","            df[col] = df[col].fillna(\"_MISSING_\").astype(str)\n","\n","        # Сохраняем количество категорий ДО преобразования в строки (после обработки NaN)\n","        n_categories_before = {col: df[col].nunique() for col in categorical_features}\n","\n","        transformers.append((\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features))\n","\n","    preprocessor = ColumnTransformer(\n","        transformers=transformers,\n","        remainder='drop',  # Колонки, не попавшие в transformers, будут исключены\n","    )\n","\n","    # Применение предобработки\n","    X = preprocessor.fit_transform(df).astype(np.float32)\n","\n","    # Проверка: сколько признаков получилось после OneHot encoding\n","    n_features_after_onehot = X.shape[1]\n","    n_numeric = len(numeric_features)\n","    n_categorical = len(categorical_features)\n","    if n_categorical > 0:\n","        # Подсчитываем количество категорий (используем сохраненные значения)\n","        n_categories = sum(n_categories_before.values())\n","        expected_features = n_numeric + n_categories\n","        if abs(n_features_after_onehot - expected_features) > 0:\n","            print(f\"Внимание: После OneHot encoding получено {n_features_after_onehot} признаков\")\n","            print(f\"   Ожидалось: {n_numeric} числовых + {n_categories} категориальных = {expected_features} признаков\")\n","        else:\n","            print(f\"После OneHot encoding: {n_numeric} числовых + {n_categories} категориальных = {n_features_after_onehot} признаков\")\n","\n","    # Обработка NaN и бесконечных значений\n","    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n","    X = np.clip(X, 0.0, 1.0)  # Гарантируем диапазон [0, 1] (защита от возможных выбросов)\n","\n","    feature_info = {\n","        'numeric_features': numeric_features,\n","        'categorical_features': categorical_features,\n","        'excluded_columns': list(exclude)\n","    }\n","\n","    print(f\"Предобработка завершена\")\n","    print(f\"   - Размерность данных: {X.shape}\")\n","    print(f\"   - Диапазон значений: [{X.min():.4f}, {X.max():.4f}]\")\n","\n","    return X, preprocessor, feature_info\n","\n","print(\"Функция предобработки определена\")"]},{"cell_type":"markdown","metadata":{"id":"e3boNXYI0_Z4"},"source":["## VAE (Variational Autoencoder) модель\n","\n","**VAE** - это генеративная модель, которая учится сжимать данные в латентное пространство меньшей размерности.\n","\n","### Математика VAE:\n","\n","**Loss функция состоит из двух частей:**\n","1. **Reconstruction Loss** (ошибка восстановления): $L_{recon} = \\sum_{i} BCE(x_i, \\hat{x}_i)$\n","   - Измеряет, насколько хорошо модель восстанавливает исходные данные\n","   \n","2. **KL Divergence** (регуляризация): $L_{KL} = -0.5 \\sum_{j} (1 + \\log(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2)$\n","   - Заставляет латентное пространство следовать стандартному нормальному распределению\n","   - Это обеспечивает гладкое и интерпретируемое латентное пространство\n","\n","**Общий loss**: $L_{total} = L_{recon} + \\beta \\cdot L_{KL}$\n","\n","Где $\\beta$ - гиперпараметр, контролирующий баланс между точностью восстановления и регулярзацией.\n"]},{"cell_type":"markdown","metadata":{"id":"iWvwf2QlkoJr"},"source":["## Методология обучения VAE\n","\n","### 1. Защита от коллапса латентного пространства\n","\n","**Проблема**: При обучении VAE может произойти \"коллапс\" латентного пространства, когда KL divergence становится очень маленьким, и модель игнорирует регуляризацию.\n","\n","**Решение**:\n","- Динамическое увеличение коэффициента β при обнаружении малого KL loss\n","- Ограничение минимального значения KL loss для предотвращения полного коллапса\n","- Мониторинг соотношения KL loss к Reconstruction loss\n","\n","### 2. Оптимизация гиперпараметров\n","\n","- **Early Stopping**: Остановка обучения при отсутствии улучшения loss\n","- **Learning Rate Scheduling**: Автоматическое уменьшение learning rate при плато\n","- **Gradient Clipping**: Предотвращение взрывающихся градиентов\n","\n","### 3. Использование латентного пространства\n","\n","После обучения VAE мы используем **среднее значение** латентного распределения (μ) для кластеризации, а не случайные выборки. Это обеспечивает:\n","- Детерминированность результатов\n","- Более стабильные и интерпретируемые представления\n","- Лучшую разделимость кластеров\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOxTr1wa0_Z4","cellView":"form"},"outputs":[],"source":["# @title\n","class VAE(nn.Module):\n","    \"\"\"\n","    Variational Autoencoder с настраиваемой архитектурой.\n","\n","    Архитектура:\n","    - Encoder: входные данные → скрытые слои → (μ, log σ²)\n","    - Reparameterization Trick: z = μ + ε·σ, где ε ~ N(0,1)\n","    - Decoder: z → скрытые слои → восстановленные данные\n","    \"\"\"\n","\n","    def __init__(self, input_dim, hidden_dims, latent_dim, dropout=0.2, use_batch_norm=True):\n","        \"\"\"\n","        Параметры:\n","        -----------\n","        input_dim : int\n","            Размерность входных данных\n","        hidden_dims : list\n","            Список размерностей скрытых слоев (например, [512, 256, 128])\n","        latent_dim : int\n","            Размерность латентного пространства\n","        dropout : float\n","            Вероятность dropout для регуляризации\n","        use_batch_norm : bool\n","            Использовать ли Batch Normalization\n","        \"\"\"\n","        super(VAE, self).__init__()\n","\n","        self.input_dim = input_dim\n","        self.latent_dim = latent_dim\n","\n","        # ========== ENCODER ==========\n","        # Сжимает данные в латентное представление\n","        encoder_layers = []\n","        prev_dim = input_dim\n","        for hidden_dim in hidden_dims:\n","            encoder_layers.append(nn.Linear(prev_dim, hidden_dim))\n","            if use_batch_norm:\n","                encoder_layers.append(nn.BatchNorm1d(hidden_dim))  # Стабилизация обучения\n","            encoder_layers.append(nn.ReLU())  # Активация\n","            if dropout > 0:\n","                encoder_layers.append(nn.Dropout(dropout))  # Регуляризация\n","            prev_dim = hidden_dim\n","\n","        self.encoder = nn.Sequential(*encoder_layers)\n","\n","        # Выходы encoder: μ (среднее) и log σ² (логарифм дисперсии)\n","        # Это параметры нормального распределения в латентном пространстве\n","        self.fc_mu = nn.Linear(prev_dim, latent_dim)      # Среднее μ\n","        self.fc_logvar = nn.Linear(prev_dim, latent_dim)  # Логарифм дисперсии log σ²\n","\n","        # ========== DECODER ==========\n","        # Восстанавливает данные из латентного представления\n","        decoder_layers = []\n","        prev_dim = latent_dim\n","        # Обратный порядок скрытых слоев\n","        for hidden_dim in reversed(hidden_dims):\n","            decoder_layers.append(nn.Linear(prev_dim, hidden_dim))\n","            if use_batch_norm:\n","                decoder_layers.append(nn.BatchNorm1d(hidden_dim))\n","            decoder_layers.append(nn.ReLU())\n","            if dropout > 0:\n","                decoder_layers.append(nn.Dropout(dropout))\n","            prev_dim = hidden_dim\n","\n","        # Финальный слой с Sigmoid (для данных в диапазоне [0, 1])\n","        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n","        decoder_layers.append(nn.Sigmoid())\n","\n","        self.decoder = nn.Sequential(*decoder_layers)\n","\n","    def encode(self, x):\n","        \"\"\"\n","        Кодирует входные данные в параметры распределения.\n","\n","        Возвращает:\n","        -----------\n","        mu : torch.Tensor\n","            Среднее латентного распределения\n","        logvar : torch.Tensor\n","            Логарифм дисперсии латентного распределения\n","        \"\"\"\n","        h = self.encoder(x)\n","        mu = self.fc_mu(h)\n","        logvar = self.fc_logvar(h)\n","        return mu, logvar\n","\n","    def reparameterize(self, mu, logvar):\n","        \"\"\"\n","        Reparameterization Trick: z = μ + ε·σ\n","\n","        Позволяет дифференцировать через случайную выборку.\n","        Вместо прямой выборки из N(μ, σ²) мы:\n","        1. Выбираем ε ~ N(0, 1)\n","        2. Вычисляем z = μ + ε·exp(0.5·log σ²)\n","\n","        Это делает процесс дифференцируемым для backpropagation.\n","        \"\"\"\n","        std = torch.exp(0.5 * logvar)  # σ = exp(0.5·log σ²)\n","        eps = torch.randn_like(std)     # ε ~ N(0, 1)\n","        return mu + eps * std           # z = μ + ε·σ\n","\n","    def decode(self, z):\n","        \"\"\"Декодирует латентное представление обратно в данные.\"\"\"\n","        return self.decoder(z)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Прямой проход через VAE.\n","\n","        Возвращает:\n","        -----------\n","        recon_x : torch.Tensor\n","            Восстановленные данные\n","        mu : torch.Tensor\n","            Среднее латентного распределения\n","        logvar : torch.Tensor\n","            Логарифм дисперсии\n","        \"\"\"\n","        mu, logvar = self.encode(x)\n","        z = self.reparameterize(mu, logvar)\n","        recon = self.decode(z)\n","        return recon, mu, logvar\n","\n","\n","def vae_loss(x, recon_x, mu, logvar, beta=1.0):\n","    \"\"\"\n","    Вычисляет loss для VAE.\n","\n","    Формула: L = L_recon + β·L_KL\n","\n","    Где:\n","    - L_recon = BCE(x, x̂) - ошибка восстановления\n","    - L_KL = -0.5·Σ(1 + log(σ²) - μ² - σ²) - KL divergence для регуляризации\n","    - β - коэффициент, контролирующий баланс\n","\n","    Параметры:\n","    -----------\n","    x : torch.Tensor\n","        Исходные данные\n","    recon_x : torch.Tensor\n","        Восстановленные данные\n","    mu : torch.Tensor\n","        Среднее латентного распределения\n","    logvar : torch.Tensor\n","        Логарифм дисперсии\n","    beta : float\n","        Коэффициент для KL divergence (β-VAE)\n","\n","    Возвращает:\n","    -----------\n","    total_loss : torch.Tensor\n","        Общий loss\n","    recon_loss : torch.Tensor\n","        Loss восстановления\n","    kl_loss : torch.Tensor\n","        KL divergence loss\n","    \"\"\"\n","    # Reconstruction Loss\n","    recon_loss = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n","\n","    logvar_clamped = torch.clamp(logvar, min=-5.0, max=5.0)\n","    kl_loss = -0.5 * torch.sum(1 + logvar_clamped - mu.pow(2) - logvar_clamped.exp())\n","    kl_loss = torch.clamp(kl_loss, min=min_kl_value)\n","\n","    # Общий loss\n","    total_loss = recon_loss + beta * kl_loss\n","\n","    return total_loss, recon_loss, kl_loss\n","\n","\n","def train_vae(X, hidden_dims=[512, 256, 128], latent_dim=15, epochs=200,\n","              batch_size=256, lr=1e-4, beta=4.0, dropout=0.2,\n","              use_batch_norm=True, verbose=True, early_stopping_patience=20):\n","    \"\"\"\n","    Обучает VAE модель.\n","\n","    Параметры:\n","    -----------\n","    X : np.ndarray\n","        Предобработанные данные (должны быть в диапазоне [0, 1])\n","    hidden_dims : list\n","        Архитектура скрытых слоев\n","    latent_dim : int\n","        Размерность латентного пространства\n","    epochs : int\n","        Количество эпох обучения\n","    batch_size : int\n","        Размер батча\n","    lr : float\n","        Learning rate\n","    beta : float\n","        Коэффициент для KL divergence (β-VAE)\n","    dropout : float\n","        Вероятность dropout\n","    use_batch_norm : bool\n","        Использовать Batch Normalization\n","    verbose : bool\n","        Выводить ли прогресс обучения\n","    early_stopping_patience : int\n","        Количество эпох без улучшения для early stopping\n","\n","    Возвращает:\n","    -----------\n","    latent_vectors : np.ndarray\n","        Латентные представления всех данных (используется μ, среднее)\n","    model : VAE\n","        Обученная модель\n","    history : dict\n","        История обучения (loss, recon_loss, kl_loss)\n","    \"\"\"\n","    # Создание DataLoader\n","    dataset = TensorDataset(torch.tensor(X, dtype=torch.float32))\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","    # Инициализация модели\n","    model = VAE(\n","        input_dim=X.shape[1],\n","        hidden_dims=hidden_dims,\n","        latent_dim=latent_dim,\n","        dropout=dropout,\n","        use_batch_norm=use_batch_norm\n","    ).to(DEVICE)\n","\n","    # Оптимизатор и scheduler\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-6\n","    )\n","\n","    # Early stopping\n","    best_loss = float('inf')\n","    patience_counter = 0\n","    best_model_state = None\n","\n","    history = {'loss': [], 'recon_loss': [], 'kl_loss': []}\n","\n","    # Переменная для динамического beta\n","    current_beta = beta\n","    kl_threshold = 0.1  # Порог для обнаружения коллапса\n","    min_kl_target = 0.5  # Минимальная целевая величина KL loss\n","    critical_kl_threshold = 0.05\n","    max_beta_multiplier = 50.0\n","\n","    # Обучение\n","    for epoch in range(epochs):\n","        model.train()\n","        epoch_loss = 0.0\n","        epoch_recon = 0.0\n","        epoch_kl = 0.0\n","\n","        for (batch_X,) in loader:\n","            batch_X = batch_X.to(DEVICE)\n","\n","            # Прямой проход\n","            recon, mu, logvar = model(batch_X)\n","            # Используем динамический beta для защиты от коллапса\n","            total_loss, recon_loss, kl_loss = vae_loss(batch_X, recon, mu, logvar, beta=current_beta)\n","\n","            # Обратный проход\n","            optimizer.zero_grad()\n","            total_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n","            optimizer.step()\n","\n","            epoch_loss += total_loss.item()\n","            epoch_recon += recon_loss.item()\n","            epoch_kl += kl_loss.item()\n","\n","        # Средние значения за эпоху\n","        avg_loss = epoch_loss / len(loader)\n","        avg_recon = epoch_recon / len(loader)\n","        avg_kl = epoch_kl / len(loader)\n","\n","        history['loss'].append(avg_loss)\n","        history['recon_loss'].append(avg_recon)\n","        history['kl_loss'].append(avg_kl)\n","\n","        scheduler.step(avg_loss)\n","\n","        # Early stopping\n","        if avg_loss < best_loss:\n","            best_loss = avg_loss\n","            patience_counter = 0\n","            best_model_state = model.state_dict().copy()\n","        else:\n","            patience_counter += 1\n","\n","        if avg_kl < kl_threshold:\n","            # Автоматически увеличиваем beta при обнаружении коллапса\n","            if avg_kl < critical_kl_threshold:\n","                kl_ratio = max(avg_kl / critical_kl_threshold, 0.001)\n","                beta_multiplier = 1.0 / (kl_ratio ** 0.5)\n","                new_beta = min(current_beta * beta_multiplier, beta * max_beta_multiplier)\n","            else:\n","                # Обычный случай: KL loss меньше порога, но не критически\n","                kl_ratio = max(avg_kl / kl_threshold, 0.01)  # Отношение текущего KL к порогу\n","                beta_multiplier = 1.0 / kl_ratio  # Множитель для beta\n","                new_beta = min(current_beta * beta_multiplier, beta * max_beta_multiplier)\n","\n","            if avg_kl < 0.01:\n","                new_beta = max(new_beta, current_beta * 2.0)\n","\n","            if new_beta > current_beta:\n","                current_beta = new_beta\n","\n","        if avg_kl <= 0.0:\n","            current_beta = beta * max_beta_multiplier\n","\n","        if verbose and ((epoch + 1) % 20 == 0 or epoch == 0 or epoch == epochs - 1):\n","            beta_info = f\" (β={current_beta:.2f})\" if current_beta > beta else \"\"\n","            print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.2f} | \"\n","                  f\"Recon: {avg_recon:.2f} | KL: {avg_kl:.2f}{beta_info}\")\n","\n","        if patience_counter >= early_stopping_patience:\n","            if verbose:\n","                print(f\"Early stopping at epoch {epoch+1}\")\n","            model.load_state_dict(best_model_state)\n","            break\n","\n","    # Получение латентных представлений (используем μ, среднее)\n","    model.eval()\n","    with torch.no_grad():\n","        all_X = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n","        mu, _ = model.encode(all_X)\n","\n","    return mu.cpu().numpy(), model, history"]},{"cell_type":"markdown","metadata":{"id":"t9FgBzXXkoJt"},"source":["## Методология обнаружения boundary кластеров\n","\n","Алгоритм обнаружения промежуточных (boundary) кластеров основан на комбинации двух метрик:\n","\n","### 1. Соотношение расстояний (Distance Ratio)\n","\n","Для каждой точки вычисляется отношение расстояний до двух ближайших центров кластеров:\n","\n","$$r = \\frac{d_1}{d_2}$$\n","\n","где:\n","- $d_1$ - расстояние до ближайшего центра кластера\n","- $d_2$ - расстояние до второго ближайшего центра\n","\n","**Интерпретация**: чем ближе $r$ к 1, тем ближе точка находится к границе между двумя кластерами.\n","\n","### 2. Энтропия соседей (Neighbor Entropy)\n","\n","Анализируется разнообразие меток кластеров среди k ближайших соседей:\n","\n","$$H = -\\sum_{i} p_i \\log p_i$$\n","\n","где $p_i$ - доля соседей из кластера $i$.\n","\n","**Интерпретация**: высокая энтропия означает смешанное окружение, что характерно для boundary точек.\n","\n","### 3. Комбинированная метрика\n","\n","Обе метрики нормализуются и комбинируются с весами:\n","\n","$$score = w_d \\cdot r_{norm} + w_e \\cdot H_{norm}$$\n","\n","По умолчанию: $w_d = 0.9$, $w_e = 0.1$.\n","\n","### 4. Группировка boundary точек\n","\n","Boundary точки группируются в кластеры двумя способами:\n","1. **По парам основных кластеров**: каждая boundary точка находится между двумя основными кластерами, поэтому логично группировать их по парам $(c_i, c_j)$\n","2. **DBSCAN**: если количество пар превышает лимит, используется DBSCAN для объединения похожих boundary точек\n"]},{"cell_type":"markdown","metadata":{"id":"wf943p0wkoJt"},"source":["## Система метрик оценки кластеризации\n","\n","Для комплексной оценки качества кластеризации используется система метрик, разделенная на две категории:\n","\n","### Стандартные метрики кластеризации\n","\n","Эти метрики вычисляются **только на основных кластерах** (без учета boundary кластеров):\n","\n","1. **Silhouette Score** (диапазон [-1, 1], чем выше, тем лучше):\n","   - Измеряет, насколько хорошо точки разделены между кластерами\n","   - Учитывает как внутрикластерное, так и межкластерное расстояние\n","\n","2. **Davies-Bouldin Index** (чем ниже, тем лучше):\n","   - Отношение внутрикластерного расстояния к межкластерному\n","   - Низкое значение означает компактные и хорошо разделенные кластеры\n","\n","3. **Calinski-Harabasz Score** (чем выше, тем лучше):\n","   - Отношение межкластерной дисперсии к внутрикластерной\n","   - Высокое значение означает хорошую разделимость кластеров\n","\n","### Специализированные boundary метрики\n","\n","1. **boundary_ratio**: доля boundary точек от общего количества (целевой диапазон 5-15%)\n","\n","2. **boundary_density**: отношение плотности основных кластеров к boundary кластерам\n","   - Значение близкое к 1 означает, что boundary точки достаточно плотные\n","\n","3. **boundary_alignment**: среднее отношение расстояний до двух ближайших центров\n","   - Значение близкое к 1 означает хорошую выравненность boundary точек\n","\n","4. **boundary_stability**: Обратная величина стандартного отклонения расстояний внутри boundary кластеров\n","   - Высокое значение означает стабильные и компактные boundary зоны\n","\n","### Замечание о пространствах\n","\n","**Метрики вычисляются на том пространстве, на котором работает метод**:\n","- VAE методы: на латентном пространстве VAE\n","- PCA+KMeans: на пространстве PCA\n","- Остальные методы: на исходном пространстве признаков\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rYTvaWKW0_Z4","cellView":"form"},"outputs":[],"source":["# @title\n","def detect_boundary_clusters(latent_vectors, cluster_labels, k_neighbors=10,\n","                             boundary_threshold=0.5, min_boundary_points=10,\n","                             max_boundary_clusters=None,\n","                             weight_distance=0.6, weight_entropy=0.4):\n","    \"\"\"\n","    Обнаруживает промежуточные (boundary) кластеры между основными кластерами.\n","\n","    Алгоритм:\n","    1. Вычисляет интегральную метрику для каждой точки (расстояния + энтропия)\n","    2. Определяет boundary точки по порогу\n","    3. Группирует boundary точки в кластеры (по парам основных кластеров или DBSCAN)\n","\n","    Параметры:\n","    -----------\n","    latent_vectors : np.ndarray\n","        Латентные представления данных (shape: [n_samples, latent_dim])\n","    cluster_labels : np.ndarray\n","        Метки основных кластеров (shape: [n_samples])\n","    k_neighbors : int\n","        Количество ближайших соседей для анализа энтропии\n","    boundary_threshold : float\n","        Порог для определения boundary точек (0-1). Чем выше, тем строже.\n","    min_boundary_points : int\n","        Минимальное количество точек для создания boundary кластера\n","    max_boundary_clusters : int, optional\n","        Максимальное количество boundary кластеров (None = автоматически)\n","    weight_distance : float\n","        Вес для метрики расстояний в комбинированной метрике (по умолчанию 0.6)\n","    weight_entropy : float\n","        Вес для метрики энтропии в комбинированной метрике (по умолчанию 0.4)\n","        Примечание: weight_distance + weight_entropy должны быть близки к 1.0\n","\n","    Возвращает:\n","    -----------\n","    boundary_mask : np.ndarray\n","        Булева маска для boundary точек\n","    boundary_labels : np.ndarray\n","        Метки для boundary кластеров (-1 для не-boundary точек)\n","    \"\"\"\n","    n_samples = len(latent_vectors)\n","    unique_clusters = np.unique(cluster_labels)\n","    n_clusters = len(unique_clusters)\n","\n","    if n_clusters < 2:\n","        return np.zeros(n_samples, dtype=bool), np.full(n_samples, -1)\n","\n","    # ========== ШАГ 1: Вычисление центров основных кластеров ==========\n","    cluster_centers = []\n","    for cluster_id in unique_clusters:\n","        mask = cluster_labels == cluster_id\n","        center = latent_vectors[mask].mean(axis=0)  # Центр = среднее точек кластера\n","        cluster_centers.append(center)\n","    cluster_centers = np.array(cluster_centers)\n","\n","    # ========== ШАГ 2: Вычисление соотношения расстояний (r = d₁/d₂) ==========\n","    # Для каждой точки находим расстояния до всех центров кластеров\n","    distances_to_centers = cdist(latent_vectors, cluster_centers)\n","\n","    # Находим два ближайших центра для каждой точки\n","    sorted_indices = np.argsort(distances_to_centers, axis=1)\n","    closest_center_dist = distances_to_centers[np.arange(n_samples), sorted_indices[:, 0]]      # d₁\n","    second_closest_center_dist = distances_to_centers[np.arange(n_samples), sorted_indices[:, 1]] # d₂\n","\n","    # Вычисляем отношение расстояний: r = d₁/d₂\n","    # Чем ближе r к 1, тем ближе точка к границе между кластерами\n","    distance_ratio = closest_center_dist / (second_closest_center_dist + 1e-10)  # +1e-10 для избежания деления на 0\n","\n","    # ========== ШАГ 3: Вычисление энтропии соседей (H = -Σ pᵢ log pᵢ) ==========\n","    # Анализ разнообразия кластеров среди k ближайших соседей\n","    nbrs = NearestNeighbors(n_neighbors=min(k_neighbors + 1, n_samples),\n","                           metric='euclidean').fit(latent_vectors)\n","    distances, indices = nbrs.kneighbors(latent_vectors)\n","\n","    neighbor_diversity = []\n","    for i in range(n_samples):\n","        # Берем метки соседей (исключаем саму точку)\n","        neighbor_labels = cluster_labels[indices[i, 1:]]\n","        unique_neighbor_labels = np.unique(neighbor_labels)\n","\n","        # Вычисляем энтропию распределения кластеров среди соседей\n","        label_counts = Counter(neighbor_labels)\n","        probs = np.array([label_counts[label] for label in unique_neighbor_labels]) / len(neighbor_labels)\n","        diversity = entropy(probs)  # H = -Σ pᵢ log pᵢ\n","        neighbor_diversity.append(diversity)\n","\n","    neighbor_diversity = np.array(neighbor_diversity)\n","\n","    # ========== ШАГ 4: Нормализация метрик и комбинирование ==========\n","    # Проверяем диапазон перед нормализацией для предотвращения деления на ноль\n","    # Нормализуем обе метрики в диапазон [0, 1]\n","    distance_range = distance_ratio.max() - distance_ratio.min()\n","    if distance_range > 1e-10:\n","        distance_ratio_norm = (distance_ratio - distance_ratio.min()) / distance_range\n","    else:\n","        # Все значения одинаковы - используем константу 0.5 (среднее значение)\n","        distance_ratio_norm = np.ones_like(distance_ratio) * 0.5\n","\n","    diversity_range = neighbor_diversity.max() - neighbor_diversity.min()\n","    if diversity_range > 1e-10:\n","        diversity_norm = (neighbor_diversity - neighbor_diversity.min()) / diversity_range\n","    else:\n","        # Все значения одинаковы - используем константу 0.5 (среднее значение)\n","        diversity_norm = np.ones_like(neighbor_diversity) * 0.5\n","\n","    # Нормализуем веса, чтобы их сумма была равна 1.0\n","    total_weight = weight_distance + weight_entropy\n","    if total_weight > 0:\n","        weight_distance_norm = weight_distance / total_weight\n","        weight_entropy_norm = weight_entropy / total_weight\n","    else:\n","        weight_distance_norm = 0.6\n","        weight_entropy_norm = 0.4\n","\n","    # Комбинированная метрика: score = weight_distance·r_norm + weight_entropy·H_norm\n","    # Высокое значение = точка находится на границе между кластерами\n","    boundary_score = (distance_ratio_norm * weight_distance_norm + diversity_norm * weight_entropy_norm)\n","\n","    # Определяем boundary точки по порогу\n","    boundary_mask = boundary_score >= boundary_threshold\n","\n","    # ========== ШАГ 5: Группировка boundary точек в кластеры ==========\n","    boundary_labels = np.full(n_samples, -1)  # -1 означает \"не boundary\"\n","\n","    if boundary_mask.sum() >= min_boundary_points:\n","        boundary_vectors = latent_vectors[boundary_mask]\n","\n","        if len(boundary_vectors) > 1:\n","            # СТРАТЕГИЯ 1: Группировка по парам ближайших основных кластеров\n","            # Каждая boundary точка находится между двумя основными кластерами\n","            # Логично группировать их по парам (cᵢ, cⱼ)\n","\n","            # Для каждой boundary точки находим два ближайших основных кластера\n","            distances_to_centers_boundary = cdist(boundary_vectors, cluster_centers)\n","            sorted_indices_boundary = np.argsort(distances_to_centers_boundary, axis=1)\n","\n","            # Создаем метки на основе пар ближайших кластеров\n","            boundary_cluster_pairs = []\n","            for i in range(len(boundary_vectors)):\n","                closest_pair = tuple(sorted(sorted_indices_boundary[i, :2]))  # Сортируем для уникальности\n","                boundary_cluster_pairs.append(closest_pair)\n","\n","            # Создаем уникальные метки для каждой уникальной пары кластеров\n","            unique_pairs = list(set(boundary_cluster_pairs))\n","            pair_to_label = {pair: idx + n_clusters for idx, pair in enumerate(unique_pairs)}\n","\n","            # Ограничиваем максимальное количество boundary кластеров\n","            if max_boundary_clusters is None:\n","                # По умолчанию: максимум не больше количества основных кластеров\n","                max_boundary_clusters = n_clusters\n","            else:\n","                # Ограничиваем сверху количеством основных кластеров\n","                max_boundary_clusters = min(max_boundary_clusters, n_clusters)\n","\n","            if len(unique_pairs) > max_boundary_clusters:\n","                # СТРАТЕГИЯ 2: DBSCAN для объединения похожих boundary точек\n","                # Если пар слишком много, используем DBSCAN\n","\n","                # Автоматический подбор eps\n","                nbrs_boundary = NearestNeighbors(n_neighbors=min(10, len(boundary_vectors))).fit(boundary_vectors)\n","                distances_boundary, _ = nbrs_boundary.kneighbors(boundary_vectors)\n","                k_distances = np.sort(distances_boundary[:, -1])\n","                eps_boundary = np.percentile(k_distances, 90)  # Больший eps = меньше кластеров\n","\n","                # min_samples для создания более крупных кластеров\n","                min_samples_boundary = max(5, len(boundary_vectors) // 20)\n","\n","                # Применяем DBSCAN\n","                dbscan = DBSCAN(eps=eps_boundary, min_samples=min_samples_boundary)\n","                boundary_cluster_labels = dbscan.fit_predict(boundary_vectors)\n","\n","                # Ограничиваем количество кластеров DBSCAN после его применения\n","                # Если DBSCAN создал больше кластеров, чем разрешено, объединяем их\n","                unique_dbscan_labels = np.unique(boundary_cluster_labels)\n","                # Исключаем шум (метка -1)\n","                valid_dbscan_labels = unique_dbscan_labels[unique_dbscan_labels != -1]\n","                n_dbscan_clusters = len(valid_dbscan_labels)\n","\n","                if n_dbscan_clusters > max_boundary_clusters:\n","                    # Объединяем кластеры, используя иерархическую кластеризацию\n","                    # Создаем центры кластеров DBSCAN\n","                    cluster_centers = []\n","                    for label in valid_dbscan_labels:\n","                        mask = boundary_cluster_labels == label\n","                        if mask.sum() > 0:\n","                            center = boundary_vectors[mask].mean(axis=0)\n","                            cluster_centers.append(center)\n","                    cluster_centers = np.array(cluster_centers)\n","\n","                    # Применяем AgglomerativeClustering для объединения кластеров\n","                    agg_clustering = AgglomerativeClustering(\n","                        n_clusters=max_boundary_clusters,\n","                        linkage='ward'\n","                    )\n","                    cluster_groups = agg_clustering.fit_predict(cluster_centers)\n","\n","                    # Создаем маппинг: старый label -> новый label\n","                    label_mapping = {}\n","                    for old_label, new_group in zip(valid_dbscan_labels, cluster_groups):\n","                        label_mapping[old_label] = new_group\n","\n","                    # Применяем маппинг к boundary_cluster_labels\n","                    boundary_cluster_labels_mapped = boundary_cluster_labels.copy()\n","                    for old_label, new_group in label_mapping.items():\n","                        boundary_cluster_labels_mapped[boundary_cluster_labels == old_label] = new_group\n","                    boundary_cluster_labels = boundary_cluster_labels_mapped\n","\n","                # Переносим метки обратно в исходный массив\n","                boundary_idx = 0\n","                for i in range(n_samples):\n","                    if boundary_mask[i]:\n","                        if boundary_cluster_labels[boundary_idx] != -1:\n","                            boundary_labels[i] = boundary_cluster_labels[boundary_idx] + n_clusters\n","                        boundary_idx += 1\n","            else:\n","                # Используем группировку по парам кластеров\n","                boundary_idx = 0\n","                for i in range(n_samples):\n","                    if boundary_mask[i]:\n","                        pair = boundary_cluster_pairs[boundary_idx]\n","                        boundary_labels[i] = pair_to_label[pair]\n","                        boundary_idx += 1\n","\n","    return boundary_mask, boundary_labels\n"]},{"cell_type":"markdown","metadata":{"id":"qnCViLcKkoJu"},"source":["## Интерпретация результатов кластеризации\n","\n","После выполнения кластеризации важно понять характеристики каждого кластера для практического применения.\n","\n","### Анализ основных кластеров\n","\n","Для каждого основного кластера определяются:\n","\n","1. **Важные числовые признаки**:\n","   - Z-score: насколько среднее значение кластера отличается от общего среднего\n","   - Процентное отклонение: на сколько процентов кластер отличается от среднего\n","   - Признаки с высоким абсолютным Z-score считаются наиболее характерными\n","\n","2. **Важные категориальные признаки**:\n","   - Наиболее частое значение в кластере (мода)\n","   - Отношение частот: во сколько раз чаще значение встречается в кластере по сравнению с общим датасетом\n","   - Высокое отношение частот (>1.5) указывает на характерность признака для кластера\n","\n","### Анализ boundary кластеров\n","\n","Boundary кластеры анализируются аналогично, но их интерпретация отличается:\n","- Они представляют **переходные зоны** между основными кластерами\n","- Их характеристики часто находятся между характеристиками соседних основных кластеров\n","- Для бизнеса это может означать пользователей, которых нельзя однозначно отнести к какой-либо группе\n","\n","### Описательная статистика\n","\n","Для числовых признаков вычисляется полная описательная статистика:\n","- Медиана, квартили (25%, 75%), среднее, стандартное отклонение\n","- Это позволяет понять распределение значений внутри каждого кластера\n"]},{"cell_type":"markdown","metadata":{"id":"pVKGsS7W0_Z5"},"source":["## Система метрик\n","\n","Вычисляет как стандартные метрики кластеризации, так и специализированные boundary-метрики.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlvA-BBp0_Z5","cellView":"form"},"outputs":[],"source":["# @title\n","def evaluate_clustering_metrics(data_space, cluster_labels, boundary_labels):\n","    \"\"\"\n","    Вычисляет комплексную систему метрик для кластеризации с boundary кластерами.\n","\n","    Метрики вычисляются на переданном пространстве данных (data_space).\n","    Это может быть исходное пространство признаков, латентное пространство VAE,\n","    пространство PCA и т.д. При сравнении методов каждый метод оценивается\n","    на своем соответствующем пространстве (метод + его пространство).\n","\n","    Основные метрики (стандартные):\n","    - Silhouette Score: мера разделимости кластеров (чем выше, тем лучше, диапазон [-1, 1])\n","    - Davies-Bouldin Index: отношение внутрикластерного расстояния к межкластерному (чем ниже, тем лучше)\n","    - Calinski-Harabasz Score: отношение межкластерной дисперсии к внутрикластерной (чем выше, тем лучше)\n","\n","    Boundary метрики (специализированные):\n","    - boundary_ratio: доля boundary-точек\n","    - boundary_density: отношение плотности основных кластеров к boundary-кластерам\n","    - boundary_alignment: среднее отношение расстояний до двух ближайших центров (выравненность)\n","    - boundary_stability: обратная величина стандартного отклонения расстояний внутри boundary-кластеров\n","\n","    Метрики вычисляются только на основных кластерах (без учета boundary кластеров).\n","\n","    Параметры:\n","    -----------\n","    data_space : np.ndarray\n","        Пространство данных для вычисления метрик (может быть исходное, латентное, PCA и т.д.)\n","    cluster_labels : np.ndarray\n","        Метки основных кластеров\n","    boundary_labels : np.ndarray\n","        Метки boundary кластеров\n","    \"\"\"\n","    metrics = {}\n","\n","    # Разделяем основные и boundary точки\n","    main_mask = boundary_labels == -1\n","    boundary_mask = boundary_labels != -1\n","\n","    main_labels = cluster_labels[main_mask].copy() if main_mask.sum() > 0 else np.array([])\n","    main_vectors = data_space[main_mask] if main_mask.sum() > 0 else np.array([]).reshape(0, data_space.shape[1])\n","\n","    # ========== ОСНОВНЫЕ МЕТРИКИ  ==========\n","    if len(np.unique(cluster_labels)) > 1 and len(main_vectors) > 0:\n","        if len(np.unique(main_labels)) > 1 and len(main_vectors) > 1:\n","            metrics['silhouette'] = silhouette_score(main_vectors, main_labels)\n","            metrics['davies_bouldin'] = davies_bouldin_score(main_vectors, main_labels)\n","            metrics['calinski_harabasz'] = calinski_harabasz_score(main_vectors, main_labels)\n","        else:\n","            metrics['silhouette'] = -1.0\n","            metrics['davies_bouldin'] = float('inf')\n","            metrics['calinski_harabasz'] = 0.0\n","    else:\n","        metrics['silhouette'] = -1.0\n","        metrics['davies_bouldin'] = float('inf')\n","        metrics['calinski_harabasz'] = 0.0\n","\n","    # ========== BOUNDARY МЕТРИКИ ==========\n","    n_boundary = boundary_mask.sum()\n","    n_total = len(cluster_labels)\n","    metrics['boundary_ratio'] = n_boundary / n_total if n_total > 0 else 0.0\n","\n","    if n_boundary > 0:\n","        boundary_vectors = data_space[boundary_mask]\n","\n","        # 1. Boundary Density: отношение плотности основных кластеров к boundary\n","        if len(boundary_vectors) > 1:\n","            boundary_distances = cdist(boundary_vectors, boundary_vectors)\n","            np.fill_diagonal(boundary_distances, np.inf)\n","            avg_boundary_distance = np.min(boundary_distances, axis=1).mean()\n","\n","            if len(main_vectors) > 1:\n","                main_distances = cdist(main_vectors, main_vectors)\n","                np.fill_diagonal(main_distances, np.inf)\n","                avg_main_distance = np.min(main_distances, axis=1).mean()\n","                # Чем ближе к 1, тем лучше (boundary точки должны быть плотными)\n","                metrics['boundary_density'] = avg_main_distance / (avg_boundary_distance + 1e-10)\n","            else:\n","                metrics['boundary_density'] = 0.0\n","        else:\n","            metrics['boundary_density'] = 0.0\n","\n","        # 2. Boundary Alignment: выравненность относительно ближайших центров\n","        unique_main_clusters = np.unique(cluster_labels[main_mask])\n","        cluster_centers = []\n","        for cluster_id in unique_main_clusters:\n","            mask = (cluster_labels == cluster_id) & main_mask\n","            center = data_space[mask].mean(axis=0)\n","            cluster_centers.append(center)\n","        cluster_centers = np.array(cluster_centers)\n","\n","        distances_to_centers = cdist(boundary_vectors, cluster_centers)\n","        sorted_indices = np.argsort(distances_to_centers, axis=1)\n","        closest_dist = distances_to_centers[np.arange(len(boundary_vectors)), sorted_indices[:, 0]]\n","        second_closest_dist = distances_to_centers[np.arange(len(boundary_vectors)), sorted_indices[:, 1]]\n","        # Чем ближе отношение к 1, тем лучше выравнена точка\n","        alignment_ratios = closest_dist / (second_closest_dist + 1e-10)\n","        metrics['boundary_alignment'] = alignment_ratios.mean()\n","\n","        # 3. Boundary Stability: стабильность boundary зон\n","        unique_boundary_clusters = np.unique(boundary_labels[boundary_mask])\n","        stability_scores = []\n","        for boundary_cluster_id in unique_boundary_clusters:\n","            mask = boundary_labels == boundary_cluster_id\n","            if mask.sum() > 1:\n","                cluster_points = data_space[mask]\n","                center = cluster_points.mean(axis=0)\n","                distances = np.linalg.norm(cluster_points - center, axis=1)\n","                # Стабильность = обратная величина стандартного отклонения\n","                stability = 1.0 / (distances.std() + 1e-10)\n","                stability_scores.append(stability)\n","        metrics['boundary_stability'] = np.mean(stability_scores) if stability_scores else 0.0\n","    else:\n","        metrics['boundary_density'] = 0.0\n","        metrics['boundary_alignment'] = 0.0\n","        metrics['boundary_stability'] = 0.0\n","\n","    return metrics\n"]},{"cell_type":"markdown","metadata":{"id":"IkqK6oCQkoJv"},"source":["## Методология сравнения методов\n","\n","Для объективной оценки нашего метода (VAE + KMeans + Boundary) мы сравниваем его с классическими методами кластеризации:\n","\n","### Baseline методы\n","\n","1. **PCA + KMeans**: линейное снижение размерности с последующей кластеризацией\n","2. **Standard KMeans**: кластеризация на исходном пространстве признаков\n","3. **DBSCAN**: плотностная кластеризация, автоматически определяющая количество кластеров\n","4. **Agglomerative Clustering**: иерархическая кластеризация с фиксированным количеством кластеров\n","\n","### Важные методологические принципы\n","\n","1. **Справедливое сравнение**:\n","   - Все методы используют одинаковое количество кластеров (определяется из результатов VAE)\n","   - Baseline методы **не используют** boundary detection, так как это часть предлагаемого метода\n","\n","2. **Пространства для метрик**:\n","   - Каждый метод оценивается на своем соответствующем пространстве\n","   - Это сравнение \"метод + его пространство\", что отражает реальную производительность подхода\n","\n","3. **Визуализация**:\n","   - Все методы визуализируются через PCA в 2D/3D для единообразия\n","   - Это позволяет визуально сравнить качество разделения кластеров\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_gvxQs60_Z5","cellView":"form"},"outputs":[],"source":["# @title\n","def analyze_cluster_characteristics(df_original, cluster_labels, boundary_labels,\n","                                   preprocessor, feature_info, top_n=5):\n","    \"\"\"\n","    Анализирует характеристики кластеров и определяет наиболее важные признаки.\n","\n","    Параметры:\n","    -----------\n","    df_original : pd.DataFrame\n","        Исходный DataFrame (до предобработки)\n","    cluster_labels : np.ndarray\n","        Метки основных кластеров\n","    boundary_labels : np.ndarray\n","        Метки boundary кластеров\n","    preprocessor : ColumnTransformer\n","        Обученный препроцессор\n","    feature_info : dict\n","        Информация о признаках\n","    top_n : int\n","        Количество топ признаков для вывода\n","\n","    Возвращает:\n","    -----------\n","    cluster_descriptions : dict\n","        Описание каждого кластера с важными признаками\n","    \"\"\"\n","    # Получаем числовые и категориальные признаки\n","    numeric_features = feature_info.get('numeric_features', [])\n","    categorical_features = feature_info.get('categorical_features', [])\n","\n","    # Создаем копию исходных данных\n","    df = df_original.copy()\n","\n","    # Исключаем колонки, которые не были использованы\n","    all_features = numeric_features + categorical_features\n","    df_analysis = df[all_features].copy() if all_features else df.copy()\n","\n","    # Добавляем метки кластеров\n","    df_analysis['_cluster'] = cluster_labels\n","    df_analysis['_boundary'] = boundary_labels\n","    df_analysis['_is_boundary'] = (boundary_labels != -1)\n","\n","    cluster_descriptions = {}\n","\n","    # Анализ основных кластеров\n","    unique_main_clusters = np.unique(cluster_labels[boundary_labels == -1])\n","\n","    for cluster_id in unique_main_clusters:\n","        mask = (cluster_labels == cluster_id) & (boundary_labels == -1)\n","        cluster_data = df_analysis[mask]\n","\n","        if len(cluster_data) == 0:\n","            continue\n","\n","        description = {\n","            'cluster_id': cluster_id,\n","            'type': 'main',\n","            'size': len(cluster_data),\n","            'percentage': len(cluster_data) / len(df_analysis) * 100,\n","            'important_features': {}\n","        }\n","\n","        # Анализ числовых признаков\n","        for feature in numeric_features:\n","            if feature not in cluster_data.columns:\n","                continue\n","\n","            cluster_mean = cluster_data[feature].mean()\n","            overall_mean = df_analysis[feature].mean()\n","            overall_std = df_analysis[feature].std()\n","\n","            if overall_std > 0:\n","                # Z-score: насколько среднее кластера отличается от общего среднего\n","                z_score = (cluster_mean - overall_mean) / overall_std\n","                # Процентное отклонение\n","                pct_diff = ((cluster_mean - overall_mean) / abs(overall_mean) * 100) if overall_mean != 0 else 0\n","\n","                description['important_features'][feature] = {\n","                    'cluster_mean': cluster_mean,\n","                    'overall_mean': overall_mean,\n","                    'z_score': z_score,\n","                    'pct_diff': pct_diff,\n","                    'type': 'numeric'\n","                }\n","\n","        # Анализ категориальных признаков\n","        for feature in categorical_features:\n","            if feature not in cluster_data.columns:\n","                continue\n","\n","            # Наиболее частое значение в кластере\n","            cluster_mode = cluster_data[feature].mode()\n","            if len(cluster_mode) > 0:\n","                mode_value = cluster_mode[0]\n","                mode_freq = (cluster_data[feature] == mode_value).sum() / len(cluster_data)\n","\n","                # Частота этого значения в общем датасете\n","                overall_freq = (df_analysis[feature] == mode_value).sum() / len(df_analysis)\n","\n","                # Отношение частот (если > 1, то признак более характерен для кластера)\n","                freq_ratio = mode_freq / (overall_freq + 1e-10)\n","\n","                description['important_features'][feature] = {\n","                    'mode_value': mode_value,\n","                    'mode_freq_in_cluster': mode_freq * 100,\n","                    'overall_freq': overall_freq * 100,\n","                    'freq_ratio': freq_ratio,\n","                    'type': 'categorical'\n","                }\n","\n","        # Сортируем признаки по важности (по абсолютному z-score для числовых, по freq_ratio для категориальных)\n","        sorted_features = sorted(\n","            description['important_features'].items(),\n","            key=lambda x: abs(x[1].get('z_score', 0)) if x[1]['type'] == 'numeric' else x[1].get('freq_ratio', 0),\n","            reverse=True\n","        )\n","\n","        description['top_features'] = [f[0] for f in sorted_features[:top_n]]\n","        cluster_descriptions[cluster_id] = description\n","\n","    # Анализ boundary кластеров\n","    unique_boundary_clusters = np.unique(boundary_labels[boundary_labels != -1])\n","\n","    for boundary_id in unique_boundary_clusters:\n","        mask = boundary_labels == boundary_id\n","        cluster_data = df_analysis[mask]\n","\n","        if len(cluster_data) == 0:\n","            continue\n","\n","        description = {\n","            'cluster_id': boundary_id,\n","            'type': 'boundary',\n","            'size': len(cluster_data),\n","            'percentage': len(cluster_data) / len(df_analysis) * 100,\n","            'important_features': {}\n","        }\n","\n","        # Анализ числовых признаков\n","        for feature in numeric_features:\n","            if feature not in cluster_data.columns:\n","                continue\n","\n","            cluster_mean = cluster_data[feature].mean()\n","            overall_mean = df_analysis[feature].mean()\n","            overall_std = df_analysis[feature].std()\n","\n","            if overall_std > 0:\n","                z_score = (cluster_mean - overall_mean) / overall_std\n","                pct_diff = ((cluster_mean - overall_mean) / abs(overall_mean) * 100) if overall_mean != 0 else 0\n","\n","                description['important_features'][feature] = {\n","                    'cluster_mean': cluster_mean,\n","                    'overall_mean': overall_mean,\n","                    'z_score': z_score,\n","                    'pct_diff': pct_diff,\n","                    'type': 'numeric'\n","                }\n","\n","        # Анализ категориальных признаков\n","        for feature in categorical_features:\n","            if feature not in cluster_data.columns:\n","                continue\n","\n","            cluster_mode = cluster_data[feature].mode()\n","            if len(cluster_mode) > 0:\n","                mode_value = cluster_mode[0]\n","                mode_freq = (cluster_data[feature] == mode_value).sum() / len(cluster_data)\n","                overall_freq = (df_analysis[feature] == mode_value).sum() / len(df_analysis)\n","                freq_ratio = mode_freq / (overall_freq + 1e-10)\n","\n","                description['important_features'][feature] = {\n","                    'mode_value': mode_value,\n","                    'mode_freq_in_cluster': mode_freq * 100,\n","                    'overall_freq': overall_freq * 100,\n","                    'freq_ratio': freq_ratio,\n","                    'type': 'categorical'\n","                }\n","\n","        sorted_features = sorted(\n","            description['important_features'].items(),\n","            key=lambda x: abs(x[1].get('z_score', 0)) if x[1]['type'] == 'numeric' else x[1].get('freq_ratio', 0),\n","            reverse=True\n","        )\n","\n","        description['top_features'] = [f[0] for f in sorted_features[:top_n]]\n","        cluster_descriptions[f'boundary_{boundary_id}'] = description\n","\n","    return cluster_descriptions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZ_6pL110_Z5","cellView":"form"},"outputs":[],"source":["# @title\n","def get_cluster_statistics(df_original, cluster_labels, boundary_labels, feature_info):\n","    \"\"\"\n","    Вычисляет описательную статистику по числовым признакам для каждого кластера.\n","\n","    Параметры:\n","    -----------\n","    df_original : pd.DataFrame\n","        Исходный DataFrame (до предобработки)\n","    cluster_labels : np.ndarray\n","        Метки основных кластеров\n","    boundary_labels : np.ndarray\n","        Метки boundary кластеров\n","    feature_info : dict\n","        Информация о признаках\n","\n","    Возвращает:\n","    -----------\n","    statistics : dict\n","        Словарь со статистикой для каждого кластера\n","    \"\"\"\n","    numeric_features = feature_info.get('numeric_features', [])\n","\n","    if not numeric_features:\n","        return {}\n","\n","    # Создаем копию исходных данных только с числовыми признаками\n","    df = df_original[numeric_features].copy()\n","\n","    # Добавляем метки кластеров\n","    df['_cluster'] = cluster_labels\n","    df['_boundary'] = boundary_labels\n","    df['_is_boundary'] = (boundary_labels != -1)\n","\n","    statistics = {}\n","\n","    # Статистика для основных кластеров\n","    unique_main_clusters = np.unique(cluster_labels[boundary_labels == -1])\n","\n","    for cluster_id in unique_main_clusters:\n","        mask = (cluster_labels == cluster_id) & (boundary_labels == -1)\n","        cluster_data = df[mask][numeric_features]\n","\n","        if len(cluster_data) == 0:\n","            continue\n","\n","        cluster_stats = {}\n","        for feature in numeric_features:\n","            if feature not in cluster_data.columns:\n","                continue\n","\n","            values = cluster_data[feature].dropna()\n","            if len(values) > 0:\n","                cluster_stats[feature] = {\n","                    'count': len(values),\n","                    'mean': values.mean(),\n","                    'std': values.std(),\n","                    'min': values.min(),\n","                    '25%': values.quantile(0.25),\n","                    'median': values.median(),\n","                    '75%': values.quantile(0.75),\n","                    'max': values.max()\n","                }\n","\n","        statistics[f'Cluster_{cluster_id}'] = {\n","            'type': 'main',\n","            'size': len(cluster_data),\n","            'features': cluster_stats\n","        }\n","\n","    # Статистика для boundary кластеров\n","    unique_boundary_clusters = np.unique(boundary_labels[boundary_labels != -1])\n","\n","    for boundary_id in unique_boundary_clusters:\n","        mask = boundary_labels == boundary_id\n","        cluster_data = df[mask][numeric_features]\n","\n","        if len(cluster_data) == 0:\n","            continue\n","\n","        cluster_stats = {}\n","        for feature in numeric_features:\n","            if feature not in cluster_data.columns:\n","                continue\n","\n","            values = cluster_data[feature].dropna()\n","            if len(values) > 0:\n","                cluster_stats[feature] = {\n","                    'count': len(values),\n","                    'mean': values.mean(),\n","                    'std': values.std(),\n","                    'min': values.min(),\n","                    '25%': values.quantile(0.25),\n","                    'median': values.median(),\n","                    '75%': values.quantile(0.75),\n","                    'max': values.max()\n","                }\n","\n","        statistics[f'Boundary_{boundary_id}'] = {\n","            'type': 'boundary',\n","            'size': len(cluster_data),\n","            'features': cluster_stats\n","        }\n","\n","    return statistics\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MIZmSG2l0_Z6","cellView":"form"},"outputs":[],"source":["# @title\n","def print_cluster_descriptions(cluster_descriptions, statistics, verbose=True):\n","    \"\"\"\n","    Выводит описание кластеров и статистику в читаемом формате.\n","\n","    Параметры:\n","    -----------\n","    cluster_descriptions : dict\n","        Описание кластеров из analyze_cluster_characteristics\n","    statistics : dict\n","        Статистика из get_cluster_statistics\n","    verbose : bool\n","        Выводить ли подробную информацию\n","    \"\"\"\n","    if not verbose:\n","        return\n","\n","    # Сортируем кластеры: сначала основные, потом boundary\n","    main_clusters = {k: v for k, v in cluster_descriptions.items() if v['type'] == 'main'}\n","    boundary_clusters = {k: v for k, v in cluster_descriptions.items() if v['type'] == 'boundary'}\n","\n","    print(\"\\n\" + \"─\"*80)\n","    print(\"ОСНОВНЫЕ КЛАСТЕРЫ\")\n","    print(\"─\"*80)\n","\n","    for cluster_id in sorted(main_clusters.keys()):\n","        desc = main_clusters[cluster_id]\n","        print(f\"\\nКластер {cluster_id}\")\n","        print(f\"   Размер: {desc['size']} объектов ({desc['percentage']:.2f}% от общего количества)\")\n","        print(f\"   Топ-{len(desc['top_features'])} важных признаков:\")\n","\n","        for i, feature in enumerate(desc['top_features'], 1):\n","            feat_info = desc['important_features'][feature]\n","            if feat_info['type'] == 'numeric':\n","                z_score = feat_info['z_score']\n","                pct_diff = feat_info['pct_diff']\n","                direction = \"выше\" if z_score > 0 else \"ниже\"\n","                print(f\"      {i}. {feature}:\")\n","                print(f\"         - Среднее значение: {feat_info['cluster_mean']:.4f} \"\n","                      f\"(общее среднее: {feat_info['overall_mean']:.4f})\")\n","                print(f\"         - Отклонение: {abs(pct_diff):.2f}% {direction} среднего\")\n","                print(f\"         - Z-score: {z_score:.2f}\")\n","            else:\n","                mode_value = feat_info['mode_value']\n","                freq_ratio = feat_info['freq_ratio']\n","                print(f\"      {i}. {feature}:\")\n","                print(f\"         - Наиболее частое значение: '{mode_value}' \"\n","                      f\"({feat_info['mode_freq_in_cluster']:.1f}% в кластере vs \"\n","                      f\"{feat_info['overall_freq']:.1f}% в целом)\")\n","                print(f\"         - Отношение частот: {freq_ratio:.2f}x\")\n","\n","    # Выводим boundary кластеры\n","    if boundary_clusters:\n","        print(\"\\n\" + \"─\"*80)\n","        print(\"BOUNDARY КЛАСТЕРЫ\")\n","        print(\"─\"*80)\n","\n","        for cluster_key in sorted(boundary_clusters.keys()):\n","            desc = boundary_clusters[cluster_key]\n","            boundary_id = desc['cluster_id']\n","            print(f\"\\nBoundary кластер {boundary_id}\")\n","            print(f\"   Размер: {desc['size']} объектов ({desc['percentage']:.2f}% от общего количества)\")\n","            print(f\"   Топ-{len(desc['top_features'])} важных признаков:\")\n","\n","            for i, feature in enumerate(desc['top_features'], 1):\n","                feat_info = desc['important_features'][feature]\n","                if feat_info['type'] == 'numeric':\n","                    z_score = feat_info['z_score']\n","                    pct_diff = feat_info['pct_diff']\n","                    direction = \"выше\" if z_score > 0 else \"ниже\"\n","                    print(f\"      {i}. {feature}:\")\n","                    print(f\"         - Среднее значение: {feat_info['cluster_mean']:.4f} \"\n","                          f\"(общее среднее: {feat_info['overall_mean']:.4f})\")\n","                    print(f\"         - Отклонение: {abs(pct_diff):.2f}% {direction} среднего\")\n","                    print(f\"         - Z-score: {z_score:.2f}\")\n","                else:\n","                    mode_value = feat_info['mode_value']\n","                    freq_ratio = feat_info['freq_ratio']\n","                    print(f\"      {i}. {feature}:\")\n","                    print(f\"         - Наиболее частое значение: '{mode_value}' \"\n","                          f\"({feat_info['mode_freq_in_cluster']:.1f}% в кластере vs \"\n","                          f\"{feat_info['overall_freq']:.1f}% в целом)\")\n","                    print(f\"         - Отношение частот: {freq_ratio:.2f}x\")\n","\n","    # Выводим описательную статистику\n","    if statistics:\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"ОПИСАТЕЛЬНАЯ СТАТИСТИКА ПО ЧИСЛОВЫМ ПРИЗНАКАМ\")\n","        print(\"=\"*80)\n","\n","        for cluster_name, cluster_stats in statistics.items():\n","            if not cluster_stats['features']:\n","                continue\n","\n","            cluster_type = \"Основной\" if cluster_stats['type'] == 'main' else \"Boundary\"\n","            print(f\"\\n{cluster_type} кластер: {cluster_name} (размер: {cluster_stats['size']})\")\n","            print(\"─\"*80)\n","\n","            # Создаем DataFrame для красивого вывода\n","            stats_data = []\n","            for feature, stats in cluster_stats['features'].items():\n","                stats_data.append({\n","                    'Признак': feature,\n","                    'Среднее': f\"{stats['mean']:.4f}\",\n","                    'Стд. откл.': f\"{stats['std']:.4f}\",\n","                    'Мин': f\"{stats['min']:.4f}\",\n","                    '25%': f\"{stats['25%']:.4f}\",\n","                    'Медиана': f\"{stats['median']:.4f}\",\n","                    '75%': f\"{stats['75%']:.4f}\",\n","                    'Макс': f\"{stats['max']:.4f}\"\n","                })\n","\n","            stats_df = pd.DataFrame(stats_data)\n","            print(stats_df.to_string(index=False))\n","\n","    print(\"\\n\" + \"=\"*80)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sy0hYIBbkoJw","executionInfo":{"status":"ok","timestamp":1766063394107,"user_tz":-180,"elapsed":14,"user":{"displayName":"Ульяна","userId":"09654476276403892729"}},"outputId":"e203e5b1-89bd-42bf-e33a-0097ce06e661","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Вспомогательная функция для создания методов сравнения определена\n"]}],"source":["# @title\n","def create_baseline_methods(X, results, random_state=None, compute_metrics=False, measure_time=False):\n","    \"\"\"\n","    Создает baseline методы для сравнения: PCA+KMeans, Standard KMeans, DBSCAN, Agglomerative, VAE+KMeans.\n","\n","    Эта функция устраняет дубликаты кода, которые повторялись в нескольких местах.\n","\n","    Параметры:\n","    -----------\n","    X : np.ndarray\n","        Предобработанные данные\n","    results : dict\n","        Результаты VAE + KMeans + Boundary метода\n","    random_state : int, optional\n","        Random state для воспроизводимости (по умолчанию использует RANDOM_STATE)\n","    compute_metrics : bool\n","        Вычислять ли метрики для каждого метода (для visualize_metrics_comparison)\n","    measure_time : bool\n","        Измерять ли время выполнения каждого метода\n","\n","    Возвращает:\n","    -----------\n","    methods_data : list\n","        Список кортежей (method_name, vectors, labels) для визуализации\n","    methods_metrics : list, optional\n","        Список словарей с метриками для каждого метода (если compute_metrics=True)\n","    \"\"\"\n","    if random_state is None:\n","        random_state = RANDOM_STATE\n","\n","    vae_latent = results['latent_vectors']\n","    vae_clusters = results['cluster_labels']\n","    n_clusters_vae = len(np.unique(vae_clusters))\n","\n","    methods_data = []\n","    methods_metrics = [] if compute_metrics else None\n","\n","    # 1. PCA + KMeans\n","    start_time = time.time() if measure_time else None\n","    pca = PCA(n_components=min(15, X.shape[1]), random_state=random_state)\n","    X_pca = pca.fit_transform(X)\n","    kmeans_pca = KMeans(n_clusters=n_clusters_vae, n_init=10, random_state=random_state)\n","    labels_pca = kmeans_pca.fit_predict(X_pca)\n","    pca_time = time.time() - start_time if measure_time else None\n","    methods_data.append(('PCA + KMeans', X_pca, labels_pca))\n","\n","    if compute_metrics:\n","        boundary_labels_pca = np.full(len(labels_pca), -1)\n","        # Метрики вычисляются на пространстве PCA (X_pca)\n","        metrics_pca = evaluate_clustering_metrics(X_pca, labels_pca, boundary_labels_pca)\n","        metrics_pca['method'] = 'PCA + KMeans'\n","        if measure_time:\n","            metrics_pca['execution_time'] = pca_time\n","        methods_metrics.append(metrics_pca)\n","\n","    # 2. Standard KMeans\n","    start_time = time.time() if measure_time else None\n","    kmeans_std = KMeans(n_clusters=n_clusters_vae, n_init=10, random_state=random_state)\n","    labels_std = kmeans_std.fit_predict(X)\n","    kmeans_time = time.time() - start_time if measure_time else None\n","    methods_data.append(('Standard KMeans', X, labels_std))\n","\n","    if compute_metrics:\n","        boundary_labels_std = np.full(len(labels_std), -1)\n","        # Метрики вычисляются на исходном пространстве (X)\n","        metrics_std = evaluate_clustering_metrics(X, labels_std, boundary_labels_std)\n","        metrics_std['method'] = 'Standard KMeans'\n","        if measure_time:\n","            metrics_std['execution_time'] = kmeans_time\n","        methods_metrics.append(metrics_std)\n","\n","    # 3. DBSCAN\n","    start_time = time.time() if measure_time else None\n","    nbrs = NearestNeighbors(n_neighbors=4).fit(X)\n","    distances, _ = nbrs.kneighbors(X)\n","    k_distances = np.sort(distances[:, -1])\n","    eps = np.percentile(k_distances, 95)\n","    dbscan = DBSCAN(eps=eps, min_samples=30)\n","    labels_dbscan = dbscan.fit_predict(X)\n","    dbscan_time = time.time() - start_time if measure_time else None\n","    methods_data.append(('DBSCAN', X, labels_dbscan))\n","\n","    if compute_metrics:\n","        boundary_labels_dbscan = np.full(len(labels_dbscan), -1)\n","        # Метрики вычисляются на исходном пространстве (X)\n","        metrics_dbscan = evaluate_clustering_metrics(X, labels_dbscan, boundary_labels_dbscan)\n","        metrics_dbscan['method'] = 'DBSCAN'\n","        if measure_time:\n","            metrics_dbscan['execution_time'] = dbscan_time\n","        methods_metrics.append(metrics_dbscan)\n","\n","    # 4. Agglomerative\n","    start_time = time.time() if measure_time else None\n","    agg = AgglomerativeClustering(n_clusters=n_clusters_vae, linkage='ward')\n","    labels_agg = agg.fit_predict(X)\n","    agg_time = time.time() - start_time if measure_time else None\n","    methods_data.append(('Agglomerative', X, labels_agg))\n","\n","    if compute_metrics:\n","        boundary_labels_agg = np.full(len(labels_agg), -1)\n","        # Метрики вычисляются на исходном пространстве (X)\n","        metrics_agg = evaluate_clustering_metrics(X, labels_agg, boundary_labels_agg)\n","        metrics_agg['method'] = 'Agglomerative'\n","        if measure_time:\n","            metrics_agg['execution_time'] = agg_time\n","        methods_metrics.append(metrics_agg)\n","\n","    # 5. VAE + KMeans (время уже измерено в run_vae_boundary_clustering)\n","    methods_data.append(('VAE + KMeans', vae_latent, vae_clusters))\n","\n","    if compute_metrics:\n","        # Используем правильные boundary_labels из results\n","        vae_boundary_labels = results.get('boundary_labels', np.full(len(vae_clusters), -1))\n","        # Метрики вычисляются на латентном пространстве VAE (vae_latent)\n","        metrics_vae = evaluate_clustering_metrics(vae_latent, vae_clusters, vae_boundary_labels)\n","        metrics_vae['method'] = 'VAE + KMeans + Boundary'\n","        if measure_time:\n","            if 'execution_time' in results and results['execution_time'] is not None:\n","                metrics_vae['execution_time'] = results['execution_time']\n","            else:\n","                metrics_vae['execution_time'] = np.nan\n","        methods_metrics.append(metrics_vae)\n","\n","    if compute_metrics:\n","        return methods_data, methods_metrics\n","    else:\n","        return methods_data\n","\n","print(\"Вспомогательная функция для создания методов сравнения определена\")\n"]},{"cell_type":"markdown","metadata":{"id":"QN7w8BuYkoJw"},"source":["## Визуализация результатов\n","\n","Визуализация играет важную роль в понимании и интерпретации результатов кластеризации.\n","\n","### Типы визуализаций\n","\n","1. **2D/3D проекции**:\n","   - Используется PCA для снижения размерности до 2D/3D\n","   - Позволяет визуально оценить качество разделения кластеров\n","   - Основные кластеры и boundary кластеры визуализируются разными цветами и маркерами\n","\n","2. **Сравнение метрик**:\n","   - Столбчатые диаграммы для сравнения методов\n","   - Показывает стандартные метрики (silhouette, Davies-Bouldin, Calinski-Harabasz)\n","\n","3. **Тепловые карты p-values**:\n","   - Визуализация статистической значимости различий между методами\n","   - Зеленый цвет = значимые различия (p < 0.05)\n","   - Серый цвет = незначимые различия\n"]},{"cell_type":"markdown","metadata":{"id":"ti9njJrckoJw"},"source":["## Workflow алгоритма кластеризации\n","\n","Главная функция `run_vae_boundary_clustering` выполняет полный pipeline кластеризации:\n","\n","### Этап 1: Предобработка данных\n","- Автоматическое определение типов признаков\n","- Нормализация и кодирование\n","- Проверка корректности данных\n","\n","### Этап 2: Оптимизация гиперпараметров (опционально)\n","- Bayesian Optimization для поиска оптимальных параметров\n","- Используется train/validation split для предотвращения переобучения\n","- Оптимизируются: `latent_dim`, `n_clusters`, `boundary_threshold`, `vae_beta`\n","- Целевая функция: максимизация silhouette score на validation set\n","\n","### Этап 3: Обучение VAE\n","- Обучение модели на всех данных (после оптимизации)\n","- Получение латентных представлений для кластеризации\n","- Мониторинг процесса обучения (loss, early stopping)\n","\n","### Этап 4: Кластеризация KMeans\n","- Применение KMeans на латентном пространстве VAE\n","- Определение основных кластеров\n","\n","### Этап 5: Обнаружение boundary кластеров\n","- Вычисление интегральной метрики для каждой точки\n","- Определение boundary точек по порогу\n","- Группировка boundary точек в кластеры\n","\n","### Этап 6: Вычисление метрик и анализ\n","- Стандартные метрики кластеризации\n","- Специализированные boundary метрики\n","- Анализ характеристик кластеров\n"]},{"cell_type":"markdown","metadata":{"id":"wKtKfZYv0_Z6"},"source":["## Визуализации\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8CwakdRM0_Z6","cellView":"form"},"outputs":[],"source":["# @title\n","def visualize_methods_comparison_2d_no_boundary(X, results):\n","    \"\"\"\n","    Визуализирует 2D кластеризацию всех методов БЕЗ boundary кластеров.\n","\n","    Показывает только основные кластеры для:\n","    - PCA + KMeans\n","    - Standard KMeans\n","    - DBSCAN\n","    - Agglomerative\n","    - VAE + KMeans (наш метод, без boundary)\n","\n","    Параметры:\n","    -----------\n","    X : np.ndarray\n","        Предобработанные данные\n","    results : dict\n","        Результаты VAE + KMeans + Boundary метода\n","    \"\"\"\n","    methods_data = create_baseline_methods(X, results)\n","\n","    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n","    axes = axes.flatten()\n","\n","    for idx, (method_name, vectors, labels) in enumerate(methods_data):\n","        if idx >= 6:\n","            break\n","\n","        ax = axes[idx]\n","\n","        # PCA для визуализации в 2D\n","        pca_2d = PCA(n_components=2, random_state=RANDOM_STATE)\n","        vectors_2d = pca_2d.fit_transform(vectors)\n","\n","        # Определяем цвета для основных кластеров\n","        unique_clusters = np.unique(labels)\n","        # Исключаем шум (метка -1 для DBSCAN)\n","        unique_clusters = unique_clusters[unique_clusters != -1]\n","\n","        main_colors = plt.cm.tab10(np.linspace(0, 1, len(unique_clusters)))\n","\n","        # Рисуем основные кластеры\n","        for i, cid in enumerate(unique_clusters):\n","            mask = labels == cid\n","            if mask.sum() > 0:\n","                ax.scatter(vectors_2d[mask, 0], vectors_2d[mask, 1],\n","                          c=[main_colors[i]], label=f'Cluster {cid}',\n","                          alpha=0.6, s=15, edgecolors='black', linewidths=0.3)\n","\n","        # Для DBSCAN показываем шум отдельно\n","        if method_name == 'DBSCAN' and -1 in labels:\n","            noise_mask = labels == -1\n","            if noise_mask.sum() > 0:\n","                ax.scatter(vectors_2d[noise_mask, 0], vectors_2d[noise_mask, 1],\n","                          c='gray', label='Noise', alpha=0.3, s=10, marker='x')\n","\n","        ax.set_xlabel('PC1', fontsize=10)\n","        ax.set_ylabel('PC2', fontsize=10)\n","        ax.set_title(method_name, fontsize=12, fontweight='bold')\n","        ax.legend(loc='upper right', fontsize=8)\n","        ax.grid(True, alpha=0.3)\n","\n","    # Скрываем последний subplot\n","    axes[5].axis('off')\n","\n","    plt.suptitle('Сравнение методов кластеризации (только основные кластеры, без boundary)',\n","                 fontsize=14, fontweight='bold', y=0.995)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return fig\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"26Hah32t0_Z6","cellView":"form"},"outputs":[],"source":["# @title\n","def visualize_metrics_comparison(X, results, comparison_df=None):\n","    \"\"\"\n","    Визуализирует сравнение метрик базовых методов (без boundary) и нашего метода.\n","\n","    Параметры:\n","    -----------\n","    X : np.ndarray\n","        Предобработанные данные\n","    results : dict\n","        Результаты VAE + KMeans + Boundary метода\n","    comparison_df : pd.DataFrame, optional\n","        DataFrame с метриками (если уже вычислен). Если None, вычисляется автоматически.\n","    \"\"\"\n","    if comparison_df is None:\n","        _, methods_metrics = create_baseline_methods(X, results, compute_metrics=True, measure_time=True)\n","        comparison_df = pd.DataFrame(methods_metrics)\n","\n","    # Основные метрики для сравнения\n","    metrics_to_plot = ['silhouette', 'davies_bouldin', 'calinski_harabasz']\n","\n","    # Создаем фигуру с subplots (3 метрики + время выполнения)\n","    fig, axes = plt.subplots(1, 4, figsize=(24, 5))\n","\n","    methods = comparison_df['method'].values\n","\n","    # 1. Silhouette Score\n","    ax = axes[0]\n","    values = comparison_df['silhouette'].values\n","    colors = ['green' if 'VAE' in m else 'steelblue' for m in methods]\n","    bars = ax.barh(methods, values, color=colors, alpha=0.7)\n","    ax.set_xlabel('Silhouette Score', fontsize=12)\n","    ax.set_title('Silhouette Score\\n(чем выше, тем лучше)', fontsize=12, fontweight='bold')\n","    ax.grid(True, alpha=0.3, axis='x')\n","    # Добавляем значения на столбцы\n","    for i, (bar, val) in enumerate(zip(bars, values)):\n","        ax.text(val + 0.01, i, f'{val:.3f}', va='center', fontsize=9)\n","\n","    # 2. Davies-Bouldin Index\n","    ax = axes[1]\n","    values = comparison_df['davies_bouldin'].values\n","    colors = ['green' if 'VAE' in m else 'steelblue' for m in methods]\n","    bars = ax.barh(methods, values, color=colors, alpha=0.7)\n","    ax.set_xlabel('Davies-Bouldin Index', fontsize=12)\n","    ax.set_title('Davies-Bouldin Index\\n(чем ниже, тем лучше)', fontsize=12, fontweight='bold')\n","    ax.grid(True, alpha=0.3, axis='x')\n","    # Добавляем значения на столбцы\n","    for i, (bar, val) in enumerate(zip(bars, values)):\n","        ax.text(val + 0.05, i, f'{val:.3f}', va='center', fontsize=9)\n","\n","    # 3. Calinski-Harabasz Score\n","    ax = axes[2]\n","    values = comparison_df['calinski_harabasz'].values\n","    colors = ['green' if 'VAE' in m else 'steelblue' for m in methods]\n","    bars = ax.barh(methods, values, color=colors, alpha=0.7)\n","    ax.set_xlabel('Calinski-Harabasz Score', fontsize=12)\n","    ax.set_title('Calinski-Harabasz Score\\n(чем выше, тем лучше)', fontsize=12, fontweight='bold')\n","    ax.grid(True, alpha=0.3, axis='x')\n","    # Добавляем значения на столбцы (форматируем большие числа)\n","    for i, (bar, val) in enumerate(zip(bars, values)):\n","        if val > 1000:\n","            label = f'{val/1000:.1f}K'\n","        else:\n","            label = f'{val:.0f}'\n","        ax.text(val + val*0.02, i, label, va='center', fontsize=9)\n","\n","    # 4. Время выполнения\n","    ax = axes[3]\n","    if 'execution_time' in comparison_df.columns:\n","        values = comparison_df['execution_time'].values\n","        values_plot = np.where(np.isnan(values), 0, values)\n","        colors = ['green' if 'VAE' in m else 'steelblue' for m in methods]\n","        bars = ax.barh(methods, values_plot, color=colors, alpha=0.7)\n","        ax.set_xlabel('Время выполнения (сек)', fontsize=12)\n","        ax.set_title('Время выполнения\\n(чем меньше, тем лучше)', fontsize=12, fontweight='bold')\n","        ax.grid(True, alpha=0.3, axis='x')\n","        # Добавляем значения на столбцы\n","        for i, (bar, val) in enumerate(zip(bars, values)):\n","            if pd.isna(val) or np.isnan(val):\n","                label = 'N/A'\n","                ax.text(0.01, i, label, va='center', fontsize=9, color='red', style='italic')\n","            else:\n","                if val > 60:\n","                    label = f'{val/60:.1f} мин'\n","                else:\n","                    label = f'{val:.2f} сек'\n","                ax.text(val + val*0.02, i, label, va='center', fontsize=9)\n","    else:\n","        ax.axis('off')\n","        ax.text(0.5, 0.5, 'Время выполнения\\nне измерено',\n","                ha='center', va='center', fontsize=12, transform=ax.transAxes)\n","\n","    plt.suptitle('Сравнение метрик кластеризации и времени выполнения',\n","                 fontsize=14, fontweight='bold', y=1.02)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return fig, comparison_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ov1L1tiKkoJx","executionInfo":{"status":"ok","timestamp":1766063394137,"user_tz":-180,"elapsed":5,"user":{"displayName":"Ульяна","userId":"09654476276403892729"}},"outputId":"80f38a24-0af0-4c56-d0a0-44c6bad1db6d","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Функция 3D визуализации методов сравнения определена\n"]}],"source":["# @title\n","def visualize_methods_comparison_3d(X, results):\n","    \"\"\"\n","    Визуализирует 3D кластеризацию всех методов.\n","\n","    Параметры:\n","    -----------\n","    X : np.ndarray\n","        Предобработанные данные\n","    results : dict\n","        Результаты VAE + KMeans + Boundary метода\n","    \"\"\"\n","    methods_data = create_baseline_methods(X, results)\n","\n","    fig = plt.figure(figsize=(24, 12))\n","\n","    for idx, (method_name, vectors, labels) in enumerate(methods_data):\n","        ax = fig.add_subplot(2, 3, idx+1, projection='3d')\n","\n","        # PCA для визуализации в 3D\n","        pca_3d = PCA(n_components=3, random_state=RANDOM_STATE)\n","        vectors_3d = pca_3d.fit_transform(vectors)\n","\n","        unique_clusters = np.unique(labels)\n","        unique_clusters = unique_clusters[unique_clusters != -1]\n","        main_colors = plt.cm.tab10(np.linspace(0, 1, len(unique_clusters)))\n","\n","        for i, cid in enumerate(unique_clusters):\n","            mask = labels == cid\n","            if mask.sum() > 0:\n","                ax.scatter(vectors_3d[mask, 0], vectors_3d[mask, 1], vectors_3d[mask, 2],\n","                          c=[main_colors[i]], label=f'Cluster {cid}',\n","                          alpha=0.6, s=15, edgecolors='black', linewidths=0.3)\n","\n","        if method_name == 'DBSCAN' and -1 in labels:\n","            noise_mask = labels == -1\n","            if noise_mask.sum() > 0:\n","                ax.scatter(vectors_3d[noise_mask, 0], vectors_3d[noise_mask, 1], vectors_3d[noise_mask, 2],\n","                          c='gray', label='Noise', alpha=0.3, s=10, marker='x')\n","\n","        ax.set_xlabel('PC1', fontsize=10)\n","        ax.set_ylabel('PC2', fontsize=10)\n","        ax.set_zlabel('PC3', fontsize=10)\n","        ax.set_title(method_name, fontsize=12, fontweight='bold')\n","        ax.legend(loc='upper right', fontsize=8)\n","\n","    plt.suptitle('Сравнение методов кластеризации в 3D', fontsize=14, fontweight='bold', y=0.995)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return fig\n","\n","print(\"Функция 3D визуализации методов сравнения определена\")\n"]},{"cell_type":"markdown","metadata":{"id":"nztpNmms0_Z6"},"source":["## Главная функция для запуска всего кода\n","\n","Универсальная функция, которая принимает DataFrame и автоматически выполняет весь процесс кластеризации.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txYkNZk10_Z6","cellView":"form"},"outputs":[],"source":["# @title\n","def run_vae_boundary_clustering(df, exclude_columns=None,\n","                                latent_dim=15, hidden_dims=[512, 256, 128],\n","                                n_clusters=6, boundary_threshold=0.5,\n","                                max_boundary_clusters=None,\n","                                vae_epochs=150, vae_beta=4.0,\n","                                vae_batch_size=256, vae_lr=1e-4, vae_dropout=0.2,\n","                                use_batch_norm=True, early_stopping_patience=20,\n","                                optimize=False, max_optimization_iterations=10,\n","                                n_clusters_range=None, verbose=True):\n","    \"\"\"\n","    Главная функция для запуска VAE + KMeans + Boundary кластеризации.\n","\n","    Параметры:\n","    -----------\n","    df : pd.DataFrame\n","        Входной DataFrame с данными\n","    exclude_columns : list, optional\n","        Колонки для исключения из обработки\n","    latent_dim : int\n","        Размерность латентного пространства VAE\n","    hidden_dims : list\n","        Архитектура скрытых слоев VAE\n","    n_clusters : int\n","        Количество основных кластеров для KMeans (используется если optimize=False или как центр диапазона)\n","    n_clusters_range : tuple, optional\n","        Диапазон количества кластеров для оптимизации (min, max).\n","        Если None, используется диапазон вокруг n_clusters: (max(2, n_clusters-2), min(n_clusters+3, 15))\n","    boundary_threshold : float\n","        Порог для определения boundary точек (0-1)\n","    max_boundary_clusters : int, optional\n","        Максимальное количество boundary кластеров.\n","        Если None, ограничивается количеством основных кластеров (n_clusters).\n","        Если указано, ограничивается сверху: min(max_boundary_clusters, n_clusters).\n","    vae_epochs : int\n","        Количество эпох обучения VAE\n","    vae_beta : float\n","        Коэффициент β для β-VAE\n","    vae_batch_size : int\n","        Размер батча для обучения VAE\n","    vae_lr : float\n","        Learning rate для обучения VAE\n","    vae_dropout : float\n","        Вероятность dropout для VAE\n","    use_batch_norm : bool\n","        Использовать ли Batch Normalization в VAE\n","    early_stopping_patience : int\n","        Количество эпох без улучшения для early stopping\n","    optimize : bool\n","        Выполнять ли Bayesian Optimization гиперпараметров\n","    max_optimization_iterations : int\n","        Максимальное количество итераций оптимизации\n","    verbose : bool\n","        Выводить ли подробную информацию\n","\n","    Возвращает:\n","    -----------\n","    results : dict\n","        Словарь с результатами\n","    \"\"\"\n","    print(\"=\"*80)\n","    print(\"ЗАПУСК VAE + KMEANS + BOUNDARY КЛАСТЕРИЗАЦИИ\")\n","    print(\"=\"*80)\n","\n","    # Инициализация времени оптимизации\n","    optimization_time = 0.0\n","\n","    # ========== ШАГ 1: Предобработка данных ==========\n","    if verbose:\n","        print(\"\\nШаг 1: Предобработка данных...\")\n","    X, preprocessor, feature_info = preprocess_dataframe(df, exclude_columns=exclude_columns)\n","\n","    # Проверка: latent_dim не должен превышать размерность данных\n","    max_latent_dim = min(X.shape[1], 25)\n","    if latent_dim > max_latent_dim:\n","        if verbose:\n","            print(f\"latent_dim={latent_dim} превышает размерность данных {X.shape[1]}. Устанавливаем latent_dim={max_latent_dim}\")\n","        latent_dim = max_latent_dim\n","\n","    # ========== ШАГ 1.5: Bayesian Optimization (если включена) ==========\n","    if optimize:\n","        if not BAYESIAN_OPT_AVAILABLE:\n","            if verbose:\n","                print(\"\\nBayesian Optimization недоступен (scikit-optimize не установлен)\")\n","                print(\"   Продолжаем с заданными гиперпараметрами...\")\n","            optimize = False\n","        else:\n","            if verbose:\n","                print(\"\\nШаг 1.5: Bayesian Optimization гиперпараметров...\")\n","                print(\"   Оптимизируются параметры VAE и boundary кластеризации:\")\n","                print(\"   - VAE: latent_dim, vae_beta\")\n","                print(\"   - Кластеризация: n_clusters\")\n","                print(\"   - Boundary detection: boundary_threshold\")\n","                print(\"   Целевая функция: максимизация silhouette score\")\n","\n","            # Определяем пространство поиска\n","            max_latent_dim_opt = min(X.shape[1], 25)\n","            min_latent_dim_opt = min(8, max_latent_dim_opt)\n","\n","            # Функция для оптимизации (минимизируем отрицательный silhouette score)\n","            # Оптимизируются параметры всей системы: VAE + кластеризация + boundary detection\n","            # Используется train/validation split для предотвращения data leakage\n","            def objective(params):\n","                opt_latent_dim = int(params[0])\n","                opt_n_clusters = int(params[1])\n","                opt_boundary_threshold = params[2]\n","                opt_vae_beta = params[3]\n","\n","                try:\n","                    # Разделяем данные на train и validation для предотвращения data leakage\n","                    # Используем 80% для обучения, 20% для валидации\n","                    X_train, X_val = train_test_split(\n","                        X,\n","                        test_size=0.2,\n","                        random_state=RANDOM_STATE,\n","                        shuffle=True\n","                    )\n","\n","                    # Обучение VAE с текущими параметрами на TRAIN данных\n","                    opt_latent_train, model_temp, _ = train_vae(\n","                        X_train,\n","                        hidden_dims=hidden_dims,\n","                        latent_dim=opt_latent_dim,\n","                        epochs=min(vae_epochs, 50),  # Меньше эпох для оптимизации\n","                        beta=opt_vae_beta,\n","                        batch_size=vae_batch_size,\n","                        lr=vae_lr,\n","                        dropout=vae_dropout,\n","                        use_batch_norm=use_batch_norm,\n","                        verbose=False\n","                    )\n","\n","                    # Кластеризация на TRAIN данных\n","                    opt_kmeans = KMeans(n_clusters=opt_n_clusters, n_init=5, random_state=RANDOM_STATE)\n","                    opt_labels_train = opt_kmeans.fit_predict(opt_latent_train)\n","\n","                    # Получаем латентные представления для VALIDATION данных\n","                    # Используем обученную модель для получения латентных векторов validation данных\n","                    model_temp.eval()\n","                    with torch.no_grad():\n","                        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(DEVICE)\n","                        mu_val, _ = model_temp.encode(X_val_tensor)\n","                        opt_latent_val = mu_val.cpu().numpy()\n","\n","                    # Кластеризация на VALIDATION данных (используем центры, обученные на train)\n","                    opt_labels_val = opt_kmeans.predict(opt_latent_val)\n","\n","                    # Boundary detection на VALIDATION данных\n","                    opt_boundary_mask_val, opt_boundary_labels_val = detect_boundary_clusters(\n","                        opt_latent_val, opt_labels_val, boundary_threshold=opt_boundary_threshold,\n","                        max_boundary_clusters=max_boundary_clusters\n","                    )\n","\n","                    # Метрики вычисляются на VALIDATION данных (opt_latent_val)\n","                    opt_metrics = evaluate_clustering_metrics(opt_latent_val, opt_labels_val, opt_boundary_labels_val)\n","\n","                    # Целевая функция: максимизируем silhouette score (минимизируем отрицательный)\n","                    score = -opt_metrics.get('silhouette', -1.0)\n","\n","                    return score\n","\n","                except Exception as e:\n","                    if verbose:\n","                        print(f\"Ошибка при оптимизации: {e}\")\n","                    return 1.0  # Плохой score при ошибке\n","\n","            # Определяем диапазон количества кластеров для оптимизации\n","            if n_clusters_range is not None:\n","                min_clusters_opt = max(2, int(n_clusters_range[0]))\n","                max_clusters_opt = min(50, int(n_clusters_range[1]))  # Ограничиваем максимум 50\n","                if min_clusters_opt >= max_clusters_opt:\n","                    if verbose:\n","                        print(f\"Внимание: некорректный диапазон кластеров. Используется диапазон вокруг n_clusters={n_clusters}\")\n","                    min_clusters_opt = max(2, n_clusters - 2)\n","                    max_clusters_opt = min(n_clusters + 3, 15)\n","            else:\n","                # Используем диапазон вокруг указанного n_clusters\n","                min_clusters_opt = max(2, n_clusters - 2)\n","                max_clusters_opt = min(n_clusters + 3, 15)\n","\n","            if verbose:\n","                print(f\"   Диапазон количества кластеров для оптимизации: [{min_clusters_opt}, {max_clusters_opt}]\")\n","\n","            # Пространство поиска\n","            dimensions = [\n","                Integer(min_latent_dim_opt, max_latent_dim_opt, name='latent_dim'),\n","                Integer(min_clusters_opt, max_clusters_opt, name='n_clusters'),\n","                Real(0.3, 0.7, name='boundary_threshold'),\n","                Real(1.0, 8.0, name='vae_beta')\n","            ]\n","\n","            # Запуск оптимизации\n","            if verbose:\n","                print(f\"   Запуск оптимизации ({max_optimization_iterations} итераций)...\")\n","\n","            optimization_start_time = time.time()\n","            result = gp_minimize(\n","                func=objective,\n","                dimensions=dimensions,\n","                n_calls=max_optimization_iterations,\n","                random_state=RANDOM_STATE,\n","                verbose=verbose\n","            )\n","            optimization_time = time.time() - optimization_start_time\n","            if verbose:\n","                print(f\"   Время оптимизации: {optimization_time:.2f} сек (исключается из общего времени)\")\n","\n","            # Обновляем параметры на оптимальные\n","            latent_dim = int(result.x[0])\n","            n_clusters = int(result.x[1])\n","            boundary_threshold = result.x[2]\n","            vae_beta = result.x[3]\n","\n","            if verbose:\n","                print(f\"\\nОптимизация завершена!\")\n","                print(f\"   Оптимальные параметры:\")\n","                print(f\"   - latent_dim: {latent_dim}\")\n","                print(f\"   - n_clusters: {n_clusters}\")\n","                print(f\"   - boundary_threshold: {boundary_threshold:.3f}\")\n","                print(f\"   - vae_beta: {vae_beta:.3f}\")\n","                print(f\"   - Лучший score: {-result.fun:.4f}\")\n","\n","    # ========== ШАГ 2: Обучение VAE ==========\n","    if verbose:\n","        print(\"\\nШаг 2: Обучение VAE...\")\n","    vae_start_time = time.time()\n","    latent_vectors, model, history = train_vae(\n","        X,\n","        hidden_dims=hidden_dims,\n","        latent_dim=latent_dim,\n","        epochs=vae_epochs,\n","        batch_size=vae_batch_size,\n","        lr=vae_lr,\n","        beta=vae_beta,\n","        dropout=vae_dropout,\n","        use_batch_norm=use_batch_norm,\n","        early_stopping_patience=early_stopping_patience,\n","        verbose=verbose\n","    )\n","    vae_time = time.time() - vae_start_time\n","    if verbose:\n","        print(f\"   Время обучения VAE: {vae_time:.2f} сек\")\n","\n","    # ========== ШАГ 3: Кластеризация KMeans ==========\n","    if verbose:\n","        print(\"\\nШаг 3: Кластеризация KMeans...\")\n","    kmeans_start_time = time.time()\n","    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=RANDOM_STATE)\n","    cluster_labels = kmeans.fit_predict(latent_vectors)\n","    kmeans_time = time.time() - kmeans_start_time\n","    if verbose:\n","        print(f\"   Время кластеризации KMeans: {kmeans_time:.2f} сек\")\n","\n","    # ========== ШАГ 4: Обнаружение boundary кластеров ==========\n","    if verbose:\n","        print(\"\\nШаг 4: Обнаружение boundary кластеров...\")\n","    boundary_start_time = time.time()\n","    boundary_mask, boundary_labels = detect_boundary_clusters(\n","        latent_vectors,\n","        cluster_labels,\n","        boundary_threshold=boundary_threshold,\n","        max_boundary_clusters=max_boundary_clusters\n","    )\n","    boundary_time = time.time() - boundary_start_time\n","    if verbose:\n","        print(f\"   Время обнаружения boundary кластеров: {boundary_time:.2f} сек\")\n","\n","    # Общее время выполнения (VAE + KMeans + Boundary)\n","    total_execution_time = vae_time + kmeans_time + boundary_time\n","    if verbose:\n","        print(f\"\\n   Общее время выполнения: {total_execution_time:.2f} сек\")\n","\n","    # ========== ШАГ 5: Вычисление метрик ==========\n","    if verbose:\n","        print(\"\\nШаг 5: Вычисление метрик...\")\n","    # Метрики вычисляются на латентном пространстве VAE (latent_vectors)\n","    metrics = evaluate_clustering_metrics(latent_vectors, cluster_labels, boundary_labels)\n","\n","    # ========== ШАГ 6: Анализ характеристик кластеров ==========\n","    if verbose:\n","        print(\"\\nШаг 6: Анализ характеристик кластеров...\")\n","    cluster_descriptions = analyze_cluster_characteristics(\n","        df, cluster_labels, boundary_labels, preprocessor, feature_info\n","    )\n","    cluster_statistics = get_cluster_statistics(\n","        df, cluster_labels, boundary_labels, feature_info\n","    )\n","\n","    # Формируем результаты\n","    results = {\n","        'latent_vectors': latent_vectors,\n","        'cluster_labels': cluster_labels,\n","        'boundary_labels': boundary_labels,\n","        'metrics': metrics,\n","        'model': model,\n","        'preprocessor': preprocessor,\n","        'feature_info': feature_info,\n","        'cluster_descriptions': cluster_descriptions,\n","        'cluster_statistics': cluster_statistics,\n","        'X_processed': X,\n","        'history': history,\n","        'execution_time': total_execution_time,\n","        'vae_time': vae_time,\n","        'kmeans_time': kmeans_time,\n","        'boundary_time': boundary_time,\n","        'optimization_time': optimization_time\n","    }\n","\n","    if verbose:\n","        print(\"\\nКластеризация завершена!\")\n","        print(f\"   - Найдено основных кластеров: {len(np.unique(cluster_labels))}\")\n","        print(f\"   - Найдено boundary кластеров: {len(np.unique(boundary_labels[boundary_labels != -1]))}\")\n","        print(f\"   - Boundary точек: {boundary_mask.sum()} ({boundary_mask.sum()/len(df)*100:.2f}%)\")\n","\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"ni0E0X-CkoJx"},"source":["## Статистическая методология сравнения методов\n","\n","Для доказательства статистической значимости различий между методами используется статистический подход:\n","\n","### 1. Тест Фридмана (Friedman Test)\n","\n","**Назначение**: проверяет, есть ли статистически значимые различия между методами в целом.\n","\n","**Применение**:\n","- Используется для связанных выборок (все методы работают на одних и тех же данных)\n","- Непараметрический тест (не требует нормальности распределения)\n","- Нулевая гипотеза: все методы эквивалентны\n","\n","### 2. Post-hoc тесты (Wilcoxon Signed-Rank Test)\n","\n","**Назначение**: попарные сравнения методов после обнаружения общих различий.\n","\n","### 3. Коррекция на множественные сравнения\n","\n","**Проблема**: при множественных попарных сравнениях возрастает вероятность ошибки I рода (ложные положительные результаты).\n","\n","**Решение**: применение коррекции Bonferroni (или FDR):\n","- Корректирует p-values с учетом количества сравнений\n","- Обеспечивает контроль уровня значимости на уровне 0.05\n","\n","### 4. Интерпретация результатов\n","\n","- **p-value < 0.05**: статистически значимые различия между методами\n","- **NaN p-value**: статистический тест невозможен (недостаточно данных, отсутствие вариативности, или идентичные результаты)\n","- Тепловые карты p-values визуализируют значимость различий между всеми парами методов\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UrI-HK8okoJx","executionInfo":{"status":"ok","timestamp":1766063394183,"user_tz":-180,"elapsed":7,"user":{"displayName":"Ульяна","userId":"09654476276403892729"}},"outputId":"d1652a04-a184-4ed6-f131-9ff2f6a308c0","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Функция визуализации истории обучения определена\n"]}],"source":["# @title\n","def plot_training_history(results, figsize=(16, 5)):\n","    \"\"\"\n","    Визуализирует историю обучения VAE модели.\n","\n","    Параметры:\n","    -----------\n","    results : dict\n","        Результаты функции run_vae_boundary_clustering (должен содержать 'history')\n","    figsize : tuple\n","        Размер фигуры\n","\n","    Возвращает:\n","    -----------\n","    fig : matplotlib.figure.Figure\n","        Объект фигуры\n","    \"\"\"\n","    if 'history' not in results:\n","        print(\"Внимание: история обучения недоступна в results\")\n","        return None\n","\n","    history = results['history']\n","\n","    if not history or len(history.get('loss', [])) == 0:\n","        print(\"Внимание: история обучения пуста\")\n","        return None\n","\n","    fig, axes = plt.subplots(1, 3, figsize=figsize)\n","\n","    epochs = range(1, len(history['loss']) + 1)\n","\n","    # 1. Общий Loss\n","    ax = axes[0]\n","    ax.plot(epochs, history['loss'], 'b-', linewidth=2, label='Total Loss')\n","    ax.set_xlabel('Эпоха', fontsize=12)\n","    ax.set_ylabel('Loss', fontsize=12)\n","    ax.set_title('Общий Loss', fontsize=14, fontweight='bold')\n","    ax.legend()\n","    ax.grid(True, alpha=0.3)\n","\n","    # 2. Reconstruction Loss и KL Loss\n","    ax = axes[1]\n","    ax.plot(epochs, history['recon_loss'], 'g-', linewidth=2, label='Reconstruction Loss')\n","    ax.plot(epochs, history['kl_loss'], 'r-', linewidth=2, label='KL Loss')\n","    ax.set_xlabel('Эпоха', fontsize=12)\n","    ax.set_ylabel('Loss', fontsize=12)\n","    ax.set_title('Reconstruction и KL Loss', fontsize=14, fontweight='bold')\n","    ax.legend()\n","    ax.grid(True, alpha=0.3)\n","\n","    # 3. Отношение KL Loss к Reconstruction Loss\n","    ax = axes[2]\n","    if len(history['recon_loss']) > 0 and np.mean(history['recon_loss']) > 0:\n","        kl_recon_ratio = np.array(history['kl_loss']) / (np.array(history['recon_loss']) + 1e-10)\n","        ax.plot(epochs, kl_recon_ratio, 'purple', linewidth=2, label='KL / Recon Ratio')\n","        ax.set_xlabel('Эпоха', fontsize=12)\n","        ax.set_ylabel('Отношение', fontsize=12)\n","        ax.set_title('Отношение KL Loss к Reconstruction Loss', fontsize=14, fontweight='bold')\n","        ax.legend()\n","        ax.grid(True, alpha=0.3)\n","    else:\n","        ax.text(0.5, 0.5, 'Недостаточно данных\\nдля визуализации',\n","               ha='center', va='center', fontsize=12, transform=ax.transAxes)\n","\n","    plt.suptitle('История обучения VAE модели', fontsize=16, fontweight='bold', y=1.02)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return fig\n","\n","print(\"Функция визуализации истории обучения определена\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFdXbrS90_Z7","cellView":"form"},"outputs":[],"source":["# @title\n","def visualize_clusters(results, method_name=\"VAE + KMeans + Boundary\"):\n","    \"\"\"\n","    Визуализирует результаты кластеризации в 2D и 3D.\n","\n","    Параметры:\n","    -----------\n","    results : dict\n","        Результаты функции run_vae_boundary_clustering\n","    method_name : str\n","        Название метода для отображения\n","    \"\"\"\n","    latent_vectors = results['latent_vectors']\n","    cluster_labels = results['cluster_labels']\n","    boundary_labels = results['boundary_labels']\n","\n","    # Применяем PCA для визуализации\n","    pca_2d = PCA(n_components=2, random_state=RANDOM_STATE)\n","    vectors_2d = pca_2d.fit_transform(latent_vectors)\n","\n","    pca_3d = PCA(n_components=3, random_state=RANDOM_STATE)\n","    vectors_3d = pca_3d.fit_transform(latent_vectors)\n","\n","    unique_main = np.unique(cluster_labels[boundary_labels == -1])\n","    unique_boundary = np.unique(boundary_labels[boundary_labels != -1])\n","\n","    main_colors = plt.cm.tab10(np.linspace(0, 1, len(unique_main)))\n","    color_map = {cid: main_colors[i] for i, cid in enumerate(unique_main)}\n","\n","    if len(unique_boundary) > 0:\n","        boundary_colors = plt.cm.Reds(np.linspace(0.4, 0.9, len(unique_boundary)))\n","        for i, cid in enumerate(unique_boundary):\n","            color_map[cid] = boundary_colors[i]\n","\n","    # 2D и 3D визуализация\n","    fig = plt.figure(figsize=(20, 8))\n","\n","    # 2D\n","    ax1 = fig.add_subplot(121)\n","    for cid in unique_main:\n","        mask = (cluster_labels == cid) & (boundary_labels == -1)\n","        if mask.sum() > 0:\n","            ax1.scatter(vectors_2d[mask, 0], vectors_2d[mask, 1],\n","                       c=[color_map[cid]], label=f'Cluster {cid}',\n","                       alpha=0.6, s=15, edgecolors='black', linewidths=0.3)\n","\n","    for cid in unique_boundary:\n","        mask = boundary_labels == cid\n","        if mask.sum() > 0:\n","            ax1.scatter(vectors_2d[mask, 0], vectors_2d[mask, 1],\n","                       c=[color_map[cid]], label=f'Boundary {cid}',\n","                       alpha=0.8, s=25, edgecolors='darkred', linewidths=1.0, marker='^')\n","\n","    ax1.set_xlabel('PC1', fontsize=12)\n","    ax1.set_ylabel('PC2', fontsize=12)\n","    ax1.set_title(f'{method_name} - 2D Visualization', fontsize=14, fontweight='bold')\n","    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n","    ax1.grid(True, alpha=0.3)\n","\n","    # 3D\n","    ax2 = fig.add_subplot(122, projection='3d')\n","    for cid in unique_main:\n","        mask = (cluster_labels == cid) & (boundary_labels == -1)\n","        if mask.sum() > 0:\n","            ax2.scatter(vectors_3d[mask, 0], vectors_3d[mask, 1], vectors_3d[mask, 2],\n","                       c=[color_map[cid]], label=f'Cluster {cid}',\n","                       alpha=0.6, s=15, edgecolors='black', linewidths=0.3)\n","\n","    for cid in unique_boundary:\n","        mask = boundary_labels == cid\n","        if mask.sum() > 0:\n","            ax2.scatter(vectors_3d[mask, 0], vectors_3d[mask, 1], vectors_3d[mask, 2],\n","                       c=[color_map[cid]], label=f'Boundary {cid}',\n","                       alpha=0.8, s=25, edgecolors='darkred', linewidths=1.0, marker='^')\n","\n","    ax2.set_xlabel('PC1', fontsize=12)\n","    ax2.set_ylabel('PC2', fontsize=12)\n","    ax2.set_zlabel('PC3', fontsize=12)\n","    ax2.set_title(f'{method_name} - 3D Visualization', fontsize=14, fontweight='bold')\n","    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return fig\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_6JBsSK8koJx","executionInfo":{"status":"ok","timestamp":1766063394270,"user_tz":-180,"elapsed":53,"user":{"displayName":"Ульяна","userId":"09654476276403892729"}},"outputId":"636125d5-7129-4980-d649-d40f096a23b2","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Функция вывода технических метрик определена\n"]}],"source":["# @title\n","def print_technical_metrics(results, comparison_df=None):\n","    \"\"\"\n","    Выводит технические метрики (время выполнения) для всех методов.\n","\n","    Параметры:\n","    -----------\n","    results : dict\n","        Результаты VAE + KMeans + Boundary метода\n","    comparison_df : pd.DataFrame, optional\n","        DataFrame с метриками всех методов (если доступен)\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"ТЕХНИЧЕСКИЕ МЕТРИКИ\")\n","    print(\"=\"*80)\n","\n","    # Метрики нашего метода\n","    if 'execution_time' in results:\n","        print(f\"\\nVAE + KMeans + Boundary метод:\")\n","        print(f\"   Время выполнения алгоритма: {results['execution_time']:.2f} сек\")\n","        if 'vae_time' in results:\n","            print(f\"      - Обучение VAE: {results['vae_time']:.2f} сек ({results['vae_time']/results['execution_time']*100:.1f}%)\")\n","        if 'kmeans_time' in results:\n","            print(f\"      - Кластеризация KMeans: {results['kmeans_time']:.2f} сек ({results['kmeans_time']/results['execution_time']*100:.1f}%)\")\n","        if 'boundary_time' in results:\n","            print(f\"      - Обнаружение boundary: {results['boundary_time']:.2f} сек ({results['boundary_time']/results['execution_time']*100:.1f}%)\")\n","\n","        # Время оптимизации\n","        if 'optimization_time' in results and results['optimization_time'] > 0:\n","            total_with_opt = results['execution_time'] + results['optimization_time']\n","            print(f\"\\n   Время оптимизации гиперпараметров: {results['optimization_time']:.2f} сек\")\n","            print(f\"   Общее время (с оптимизацией): {total_with_opt:.2f} сек\")\n","            print(f\"   Примечание: время оптимизации исключено из метрик времени выполнения\")\n","\n","    # Метрики baseline методов\n","    if comparison_df is not None and 'execution_time' in comparison_df.columns:\n","        print(f\"\\nBaseline методы:\")\n","        for _, row in comparison_df.iterrows():\n","            method = row['method']\n","            time_val = row['execution_time']\n","            if pd.notna(time_val):\n","                if time_val > 60:\n","                    time_str = f\"{time_val/60:.1f} мин\"\n","                else:\n","                    time_str = f\"{time_val:.2f} сек\"\n","                print(f\"   - {method:25s}: {time_str}\")\n","\n","        # Сравнение\n","        if 'execution_time' in results and 'execution_time' in comparison_df.columns:\n","            vae_time = results['execution_time']\n","            # Исключаем VAE методы из сравнения baseline\n","            baseline_times = comparison_df[\n","                (~comparison_df['method'].str.contains('VAE', na=False))\n","            ]['execution_time'].dropna()\n","            if len(baseline_times) > 0:\n","                avg_baseline_time = baseline_times.mean()\n","                min_baseline_time = baseline_times.min()\n","                max_baseline_time = baseline_times.max()\n","\n","                print(f\"\\nСравнение:\")\n","                print(f\"   - Среднее время baseline методов: {avg_baseline_time:.2f} сек\")\n","                print(f\"   - Наш метод: {vae_time:.2f} сек\")\n","                if vae_time > 0:\n","                    ratio = vae_time / avg_baseline_time\n","                    print(f\"   - Отношение: {ratio:.2f}x {'(медленнее)' if ratio > 1 else '(быстрее)'}\")\n","\n","print(\"Функция вывода технических метрик определена\")\n"]},{"cell_type":"markdown","metadata":{"id":"WhlZAXF80_Z7"},"source":["## Сравнение с базовыми методами и статистический анализ\n","\n","Сравнение нашего метода (VAE + KMeans + Boundary) с классическими методами кластеризации и проверка статистической значимости различий.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8pTHVLU0_Z7","cellView":"form"},"outputs":[],"source":["# @title\n","def compare_with_baselines(X, results, n_runs=10, vae_params=None):\n","    \"\"\"\n","    Сравнивает VAE + KMeans + Boundary метод с baseline методами.\n","\n","    Выполняет несколько запусков для статистической значимости.\n","\n","    Параметры:\n","    -----------\n","    X : np.ndarray\n","        Предобработанные данные (можно получить из results['preprocessor'].transform(df))\n","        или передать напрямую предобработанные данные\n","    results : dict\n","        Результаты VAE + KMeans + Boundary метода. Должен содержать 'X_processed' для переобучения VAE.\n","    n_runs : int\n","        Количество запусков для каждого метода (для статистики)\n","    vae_params : dict, optional\n","        Параметры для обучения VAE. Если None, используются параметры из results или значения по умолчанию.\n","        Используются предобработанные данные из results['X_processed'] для гарантии одинаковой предобработки.\n","\n","    Возвращает:\n","    -----------\n","    comparison_df : pd.DataFrame\n","        DataFrame с метриками всех методов\n","    all_metrics_runs : dict\n","        Словарь со всеми запусками для статистического анализа\n","    \"\"\"\n","\n","\n","\n","    all_metrics_runs = {\n","        'VAE + KMeans + Boundary': [],\n","        'VAE Latent Space': [],\n","        'PCA + KMeans': [],\n","        'Standard KMeans': [],\n","        'DBSCAN': [],\n","        'Agglomerative': []\n","    }\n","\n","    # Используем результаты VAE метода для определения количества кластеров\n","    vae_latent = results['latent_vectors']\n","    vae_clusters = results['cluster_labels']\n","    vae_boundary = results['boundary_labels']\n","\n","    # Определяем количество кластеров из результатов VAE\n","    n_clusters_vae = len(np.unique(vae_clusters))\n","\n","    # Используем предобработанные данные из results для переобучения VAE\n","    if 'X_processed' in results and results['X_processed'] is not None:\n","\n","        # Определяем параметры для VAE из results или используем значения по умолчанию\n","        if vae_params is None:\n","            vae_params = {\n","                'latent_dim': results.get('latent_dim', 15),\n","                'n_clusters': n_clusters_vae,\n","                'boundary_threshold': results.get('boundary_threshold', 0.5),\n","                'vae_epochs': results.get('vae_epochs', 150),\n","                'vae_beta': results.get('vae_beta', 4.0),\n","                'optimize': False,  # Отключаем оптимизацию для ускорения\n","                'verbose': False  # Отключаем подробный вывод для ускорения\n","            }\n","\n","        X_processed = results['X_processed']\n","\n","        # Запускаем VAE метод несколько раз с переобучением\n","        global RANDOM_STATE\n","        original_random_state = RANDOM_STATE\n","\n","        for run in range(n_runs):\n","            # Устанавливаем random_state для разнообразия\n","            current_random_state = original_random_state + run\n","            RANDOM_STATE = current_random_state\n","            np.random.seed(current_random_state)\n","            torch.manual_seed(current_random_state)\n","            if torch.cuda.is_available():\n","                torch.cuda.manual_seed(current_random_state)\n","\n","            if run % 2 == 0 or run == n_runs - 1:\n","                print(f\"  VAE метод: Запуск {run+1}/{n_runs}...\")\n","\n","            try:\n","\n","                # Обучение VAE\n","                latent_vectors, model, history = train_vae(\n","                    X_processed,\n","                    hidden_dims=vae_params.get('hidden_dims', [512, 256, 128]),\n","                    latent_dim=vae_params['latent_dim'],\n","                    epochs=vae_params['vae_epochs'],\n","                    batch_size=vae_params.get('vae_batch_size', 256),\n","                    lr=vae_params.get('vae_lr', 1e-4),\n","                    beta=vae_params['vae_beta'],\n","                    dropout=vae_params.get('vae_dropout', 0.2),\n","                    use_batch_norm=vae_params.get('use_batch_norm', True),\n","                    early_stopping_patience=vae_params.get('early_stopping_patience', 20),\n","                    verbose=False\n","                )\n","\n","                # Кластеризация KMeans\n","                kmeans = KMeans(n_clusters=vae_params['n_clusters'], n_init=10, random_state=current_random_state)\n","                cluster_labels = kmeans.fit_predict(latent_vectors)\n","\n","                # Обнаружение boundary кластеров\n","                boundary_mask, boundary_labels = detect_boundary_clusters(\n","                    latent_vectors,\n","                    cluster_labels,\n","                    boundary_threshold=vae_params['boundary_threshold'],\n","                    weight_distance=vae_params.get('weight_distance', 0.5),\n","                    weight_entropy=vae_params.get('weight_entropy', 0.5)\n","                )\n","\n","                # Вычисление метрик\n","                metrics = evaluate_clustering_metrics(latent_vectors, cluster_labels, boundary_labels)\n","\n","                # Добавляем результаты VAE + KMeans + Boundary\n","                vae_metrics = metrics.copy()\n","                vae_metrics['method'] = 'VAE + KMeans + Boundary'\n","                all_metrics_runs['VAE + KMeans + Boundary'].append(vae_metrics)\n","\n","                # Добавляем результаты VAE Latent Space\n","                vae_latent_no_boundary = np.full(len(boundary_labels), -1)\n","                vae_latent_metrics = evaluate_clustering_metrics(\n","                    latent_vectors,\n","                    cluster_labels,\n","                    vae_latent_no_boundary\n","                )\n","                vae_latent_metrics['method'] = 'VAE Latent Space'\n","                all_metrics_runs['VAE Latent Space'].append(vae_latent_metrics)\n","\n","            except Exception as e:\n","                # Продолжаем с другими запусками\n","                pass\n","\n","        # Восстанавливаем исходный random_state\n","        RANDOM_STATE = original_random_state\n","        np.random.seed(original_random_state)\n","        torch.manual_seed(original_random_state)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed(original_random_state)\n","    else:\n","\n","        vae_metrics = results['metrics'].copy()\n","        vae_metrics['method'] = 'VAE + KMeans + Boundary'\n","        if 'execution_time' in results:\n","            vae_metrics['execution_time'] = results['execution_time']\n","        all_metrics_runs['VAE + KMeans + Boundary'].append(vae_metrics)\n","\n","        vae_latent_no_boundary = np.full(len(results['boundary_labels']), -1)\n","        vae_latent_metrics = evaluate_clustering_metrics(\n","            results['latent_vectors'],\n","            results['cluster_labels'],\n","            vae_latent_no_boundary\n","        )\n","        vae_latent_metrics['method'] = 'VAE Latent Space'\n","        if 'vae_time' in results:\n","            vae_latent_metrics['execution_time'] = results['vae_time']\n","        elif 'execution_time' in results:\n","            vae_latent_metrics['execution_time'] = results['execution_time'] * 0.9\n","        else:\n","            vae_latent_metrics['execution_time'] = 0.0\n","        all_metrics_runs['VAE Latent Space'].append(vae_latent_metrics)\n","\n","    for run in range(n_runs):\n","        np.random.seed(RANDOM_STATE + run)\n","        if run % 2 == 0 or run == n_runs - 1:\n","            print(f\"  Запуск {run+1}/{n_runs}...\")\n","\n","        # 3. PCA + KMeans\n","        start_time = time.time()\n","        pca = PCA(n_components=min(15, X.shape[1]), random_state=RANDOM_STATE + run)\n","        X_pca = pca.fit_transform(X)\n","        kmeans_pca = KMeans(n_clusters=n_clusters_vae, n_init=10,\n","                           random_state=RANDOM_STATE + run)\n","        labels_pca = kmeans_pca.fit_predict(X_pca)\n","        # Не применяем boundary detection к baseline методам\n","        boundary_labels_pca = np.full(len(labels_pca), -1)\n","        pca_time = time.time() - start_time\n","        # Метрики вычисляются на пространстве PCA (X_pca)\n","        metrics_pca = evaluate_clustering_metrics(X_pca, labels_pca, boundary_labels_pca)\n","        metrics_pca['method'] = 'PCA + KMeans'\n","        metrics_pca['execution_time'] = pca_time\n","        all_metrics_runs['PCA + KMeans'].append(metrics_pca)\n","\n","        # 4. Standard KMeans\n","        start_time = time.time()\n","        kmeans_std = KMeans(n_clusters=n_clusters_vae, n_init=10,\n","                          random_state=RANDOM_STATE + run)\n","        labels_std = kmeans_std.fit_predict(X)\n","        # Не применяем boundary detection к baseline методам\n","        boundary_labels_std = np.full(len(labels_std), -1)\n","        kmeans_time = time.time() - start_time\n","        # Метрики вычисляются на исходном пространстве (X)\n","        metrics_std = evaluate_clustering_metrics(X, labels_std, boundary_labels_std)\n","        metrics_std['method'] = 'Standard KMeans'\n","        metrics_std['execution_time'] = kmeans_time\n","        all_metrics_runs['Standard KMeans'].append(metrics_std)\n","\n","        # 5. DBSCAN\n","        start_time = time.time()\n","        nbrs = NearestNeighbors(n_neighbors=4).fit(X)\n","        distances, _ = nbrs.kneighbors(X)\n","        k_distances = np.sort(distances[:, -1])\n","        eps = np.percentile(k_distances, 95)\n","        dbscan = DBSCAN(eps=eps, min_samples=30)\n","        labels_dbscan = dbscan.fit_predict(X)\n","        dbscan_time = time.time() - start_time\n","\n","        # Для DBSCAN boundary detection не применяем\n","        boundary_labels_dbscan = np.full(len(labels_dbscan), -1)\n","        # Метрики вычисляются на исходном пространстве (X)\n","        metrics_dbscan = evaluate_clustering_metrics(X, labels_dbscan, boundary_labels_dbscan)\n","        metrics_dbscan['method'] = 'DBSCAN'\n","        metrics_dbscan['execution_time'] = dbscan_time\n","        all_metrics_runs['DBSCAN'].append(metrics_dbscan)\n","\n","        # 6. Agglomerative Clustering\n","        start_time = time.time()\n","        agg = AgglomerativeClustering(n_clusters=n_clusters_vae, linkage='ward')\n","        labels_agg = agg.fit_predict(X)\n","        # Не применяем boundary detection к baseline методам\n","        boundary_labels_agg = np.full(len(labels_agg), -1)\n","        agg_time = time.time() - start_time\n","        # Метрики вычисляются на исходном пространстве (X)\n","        metrics_agg = evaluate_clustering_metrics(X, labels_agg, boundary_labels_agg)\n","        metrics_agg['method'] = 'Agglomerative'\n","        metrics_agg['execution_time'] = agg_time\n","        all_metrics_runs['Agglomerative'].append(metrics_agg)\n","\n","    # Создаем DataFrame со средними значениями\n","    comparison_data = []\n","    for method, metrics_list in all_metrics_runs.items():\n","        # Вычисляем средние значения метрик\n","        avg_metrics = {}\n","        for key in metrics_list[0].keys():\n","            if key != 'method':\n","                values = [m[key] for m in metrics_list if key in m]\n","                if values:\n","                    avg_metrics[key] = np.mean(values)\n","        avg_metrics['method'] = method\n","        comparison_data.append(avg_metrics)\n","\n","    comparison_df = pd.DataFrame(comparison_data)\n","\n","\n","\n","    return comparison_df, all_metrics_runs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3WXqzAd-0_Z7","cellView":"form"},"outputs":[],"source":["# @title\n","def statistical_comparison(all_metrics_runs, metrics_to_compare=None, correction_method='bonferroni'):\n","    \"\"\"\n","    Выполняет статистическое сравнение методов с помощью теста Фридмана и post-hoc тестов.\n","\n","    Правильный статистический подход для сравнения нескольких методов кластеризации:\n","    1. Тест Фридмана (Friedman test) - проверяет, есть ли различия между методами в целом\n","    2. Post-hoc тесты (Wilcoxon signed-rank) - попарные сравнения для связанных выборок\n","    3. Коррекция на множественные сравнения (Bonferroni или FDR) - уменьшает вероятность ошибки I рода\n","\n","    Почему не Mann-Whitney U test:\n","    - Mann-Whitney U предназначен для независимых выборок\n","    - У нас связанные выборки (все методы работают на одних и тех же данных)\n","    - Множественные сравнения требуют коррекции\n","\n","    Параметры:\n","    -----------\n","    all_metrics_runs : dict\n","        Словарь со всеми запусками методов (из compare_with_baselines)\n","    metrics_to_compare : list, optional\n","        Список метрик для сравнения. Если None, используются основные метрики.\n","    correction_method : str, optional\n","        Метод коррекции на множественные сравнения: 'bonferroni', 'fdr_bh', 'fdr_by', 'holm'\n","        По умолчанию 'bonferroni' (самый консервативный)\n","\n","    Возвращает:\n","    -----------\n","    pvalue_matrices : dict\n","        Словарь с матрицами p-values для каждой метрики (с коррекцией)\n","        Примечание: NaN значения в матрице означают, что статистический тест\n","        невозможен для данной пары методов (недостаточно данных, отсутствие\n","        вариативности, или идентичные результаты)\n","    friedman_results : dict\n","        Словарь с результатами теста Фридмана для каждой метрики\n","    \"\"\"\n","    if metrics_to_compare is None:\n","        # Основные метрики для сравнения\n","        metrics_to_compare = [\n","            'silhouette',\n","            'davies_bouldin',\n","            'calinski_harabasz',\n","            'boundary_ratio',\n","            'boundary_density',\n","            'boundary_stability'\n","        ]\n","\n","    methods = list(all_metrics_runs.keys())\n","\n","    # Создаем матрицы p-values для каждой метрики\n","    pvalue_matrices = {}\n","    friedman_results = {}\n","\n","    print(\"=\"*80)\n","    print(\"СТАТИСТИЧЕСКОЕ СРАВНЕНИЕ МЕТОДОВ\")\n","    print(\"Тест Фридмана + Wilcoxon signed-rank (post-hoc) с коррекцией на множественные сравнения\")\n","    print(\"=\"*80)\n","\n","    if not MULTIPLE_TESTING_AVAILABLE:\n","        correction_method = None\n","\n","    for metric in metrics_to_compare:\n","        available_methods = []\n","        for method in methods:\n","            if len(all_metrics_runs[method]) > 0 and metric in all_metrics_runs[method][0]:\n","                if len(all_metrics_runs[method]) >= 2:\n","                    available_methods.append(method)\n","                elif len(all_metrics_runs[method]) == 1:\n","                    pass\n","\n","        if len(available_methods) < 2:\n","            continue\n","\n","        # Создаем матрицу p-values\n","        pvalue_matrix = pd.DataFrame(\n","            index=available_methods,\n","            columns=available_methods,\n","            dtype=float\n","        )\n","\n","        # Шаг 1: Подготовка данных для теста Фридмана\n","        # Преобразуем данные в матрицу: строки = запуски, столбцы = методы\n","        data_matrix = []\n","        valid_runs = None\n","\n","        for method in available_methods:\n","            values = [m[metric] for m in all_metrics_runs[method]\n","                     if metric in m and not (np.isnan(m[metric]) or np.isinf(m[metric]))]\n","            if valid_runs is None:\n","                valid_runs = len(values)\n","            elif len(values) != valid_runs:\n","                # Если количество запусков различается, используем минимальное\n","                valid_runs = min(valid_runs, len(values))\n","            data_matrix.append(values[:valid_runs] if len(values) >= valid_runs else values)\n","\n","        # Транспонируем: строки = запуски, столбцы = методы\n","        data_matrix = np.array(data_matrix).T  # shape: (n_runs, n_methods)\n","\n","        if data_matrix.shape[0] < 2 or data_matrix.shape[1] < 2:\n","            continue\n","\n","        # Шаг 2: Тест Фридмана\n","        try:\n","            friedman_stat, friedman_pvalue = friedmanchisquare(*data_matrix.T)\n","            friedman_results[metric] = {\n","                'statistic': friedman_stat,\n","                'pvalue': friedman_pvalue,\n","                'significant': friedman_pvalue < 0.05\n","            }\n","        except Exception as e:\n","            friedman_results[metric] = {'statistic': np.nan, 'pvalue': np.nan, 'significant': False}\n","\n","        # Шаг 3: Post-hoc попарные сравнения (Wilcoxon signed-rank test для связанных выборок)\n","        # Собираем все p-values для коррекции\n","        pairwise_pvalues = []\n","        pairwise_pairs = []\n","\n","        for i, method1 in enumerate(available_methods):\n","            for j, method2 in enumerate(available_methods):\n","                if i == j:\n","                    pvalue_matrix.loc[method1, method2] = 1.0  # Сравнение с самим собой\n","                elif j > i:  # Вычисляем только для верхнего треугольника\n","                    # Извлекаем значения метрики для обоих методов (связанные выборки)\n","                    values1 = data_matrix[:, i]  # Значения для method1 по всем запускам\n","                    values2 = data_matrix[:, j]  # Значения для method2 по всем запускам\n","\n","                    # Удаляем NaN и Inf значения\n","                    mask = ~(np.isnan(values1) | np.isnan(values2) | np.isinf(values1) | np.isinf(values2))\n","                    values1_clean = values1[mask]\n","                    values2_clean = values2[mask]\n","\n","                    if len(values1_clean) < 2:\n","                        pvalue = np.nan\n","                    else:\n","                        # Проверяем вариативность\n","                        diff = values1_clean - values2_clean\n","                        std_diff = np.std(diff)\n","\n","                        if std_diff == 0:\n","\n","                            if abs(np.mean(diff)) < 1e-10:\n","                                pvalue = 1.0\n","                            else:\n","                                pvalue = np.nan\n","\n","                        else:\n","                            # Wilcoxon signed-rank test для связанных выборок\n","                            try:\n","                                # Проверяем вариативность перед тестом\n","                                if len(values1_clean) != len(values2_clean):\n","                                    pvalue = np.nan\n","                                elif len(values1_clean) < 2:\n","                                    pvalue = np.nan\n","                                else:\n","                                    # Вычисляем разности для проверки\n","                                    diff = values1_clean - values2_clean\n","                                    if np.std(diff) < 1e-10:\n","                                        # Нет различий между методами\n","                                        pvalue = 1.0\n","                                    else:\n","                                        statistic, pvalue = wilcoxon(values1_clean, values2_clean, alternative='two-sided')\n","                            except ValueError as e:\n","                                if \"identical\" in str(e).lower() or \"zero\" in str(e).lower():\n","                                    pvalue = 1.0\n","                                else:\n","                                    pvalue = np.nan\n","                            except Exception as e:\n","                                pvalue = np.nan\n","\n","                    pairwise_pvalues.append(pvalue)\n","                    pairwise_pairs.append((i, j, method1, method2))\n","\n","        # Шаг 4: Коррекция на множественные сравнения\n","        correction_applied = False\n","\n","        if correction_method and MULTIPLE_TESTING_AVAILABLE and len(pairwise_pvalues) > 0:\n","            valid_indices = [idx for idx, p in enumerate(pairwise_pvalues) if not np.isnan(p)]\n","            if len(valid_indices) > 0:\n","                valid_pvalues = [pairwise_pvalues[idx] for idx in valid_indices]\n","                try:\n","                    _, corrected_pvalues, _, _ = multipletests(\n","                        valid_pvalues,\n","                        alpha=0.05,\n","                        method=correction_method\n","                    )\n","\n","\n","                    # Заменяем скорректированные p-values\n","                    for idx, corrected_p in zip(valid_indices, corrected_pvalues):\n","                        pairwise_pvalues[idx] = corrected_p\n","                    correction_applied = True\n","                except Exception as e:\n","                    pass\n","\n","        n_nan_pvalues = 0\n","        for (i, j, method1, method2), pvalue in zip(pairwise_pairs, pairwise_pvalues):\n","            if np.isnan(pvalue):\n","                n_nan_pvalues += 1\n","                pvalue_matrix.loc[method1, method2] = np.nan\n","                pvalue_matrix.loc[method2, method1] = np.nan\n","            else:\n","                pvalue_matrix.loc[method1, method2] = pvalue\n","                pvalue_matrix.loc[method2, method1] = pvalue\n","\n","        pvalue_matrices[metric] = pvalue_matrix\n","\n","\n","\n","    return pvalue_matrices, friedman_results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8tIq-5P0_Z7","cellView":"form"},"outputs":[],"source":["# @title\n","def plot_pvalue_heatmap(pvalue_matrices, significance_level=0.05, figsize=(18, 5)):\n","    \"\"\"\n","    Создает 3 тепловые карты p-values для метрик: silhouette, davies_bouldin, calinski_harabasz.\n","\n","    Параметры:\n","    -----------\n","    pvalue_matrices : dict\n","        Словарь с матрицами p-values для каждой метрики\n","    significance_level : float\n","        Уровень значимости (обычно 0.05)\n","    figsize : tuple\n","        Размер фигуры\n","\n","    Возвращает:\n","    -----------\n","    fig : matplotlib.figure.Figure\n","        Объект фигуры\n","    \"\"\"\n","    # Метрики для отображения\n","    metrics_to_plot = ['silhouette', 'davies_bouldin', 'calinski_harabasz']\n","\n","    # Проверяем, какие метрики доступны\n","    available_metrics = [m for m in metrics_to_plot if m in pvalue_matrices]\n","\n","    if len(available_metrics) == 0:\n","        raise ValueError(\"Нет доступных матриц p-values для отображения\")\n","\n","\n","    fig, axes = plt.subplots(1, 3, figsize=figsize)\n","\n","    # Если метрик меньше 3, скрываем лишние subplots\n","    for idx in range(len(available_metrics), 3):\n","        axes[idx].axis('off')\n","\n","    # Создаем тепловую карту для каждой метрики\n","    for idx, metric in enumerate(available_metrics):\n","        ax = axes[idx]\n","        pvalue_matrix = pvalue_matrices[metric].copy()\n","\n","        n_nan_in_matrix = 0\n","\n","        if pvalue_matrix.empty or pvalue_matrix.isna().all().all():\n","            print(f\"   Матрица для метрики '{metric}' пустая или содержит только NaN. Пропускаем.\")\n","            ax.axis('off')\n","            ax.text(0.5, 0.5, f'Нет данных\\nдля {metric}',\n","                   ha='center', va='center', fontsize=12)\n","            continue\n","\n","        pvalue_matrix = pvalue_matrix.clip(lower=0.0)\n","\n","        annotations = pvalue_matrix.copy().astype(str)\n","        n_nan_in_matrix = 0\n","        for i in range(len(pvalue_matrix)):\n","            for j in range(len(pvalue_matrix.columns)):\n","                pval = pvalue_matrix.iloc[i, j]\n","                if pd.isna(pval):\n","                    n_nan_in_matrix += 1\n","                    if i == j:\n","                        annotations.iloc[i, j] = '-'\n","                    else:\n","                        annotations.iloc[i, j] = 'N/A'\n","                elif i == j:\n","                    annotations.iloc[i, j] = '-'\n","                else:\n","                    # Показываем только \"<0.05\" или \">=0.05\"\n","                    if pval < significance_level:\n","                        annotations.iloc[i, j] = '<0.05'\n","                    else:\n","                        annotations.iloc[i, j] = '>=0.05'\n","\n","\n","\n","        from matplotlib.colors import ListedColormap, BoundaryNorm\n","\n","        if pvalue_matrix.empty or pvalue_matrix.isna().all().all():\n","            max_pval = 1.0\n","        else:\n","            max_pval = np.nanmax(pvalue_matrix.values) if pvalue_matrix.size > 0 else 1.0\n","            if np.isnan(max_pval) or np.isinf(max_pval):\n","                max_pval = 1.0\n","        max_display = max(1.0, max_pval * 1.1)\n","        bounds = [0.0, significance_level, 1.0, max_display]\n","        # 3 цвета: зеленый для значимых, серый для незначимых, белый для диагонали\n","        colors_list = ['#27ae60', '#95a5a6', '#ecf0f1']\n","        cmap_custom = ListedColormap(colors_list)\n","        # BoundaryNorm: для каждого интервала между границами используется соответствующий цвет\n","        # Интервал [0, 0.05) -> цвет 0 (зеленый)\n","        # Интервал [0.05, 1.0) -> цвет 1 (серый)\n","        # Интервал [1.0, max_display] -> цвет 1 (серый, для значений > 1.0 после коррекции)\n","        norm = BoundaryNorm(bounds, len(colors_list), clip=True)\n","\n","        sns.heatmap(\n","            pvalue_matrix,\n","            annot=annotations,\n","            fmt='',\n","            cmap=cmap_custom,\n","            norm=norm,\n","            vmin=0,\n","            vmax=max_display,\n","            square=True,\n","            linewidths=1.0,\n","            cbar_kws={'label': 'p-value', 'ticks': [0, significance_level, 1.0]},\n","            annot_kws={'size': 8},\n","            ax=ax,\n","            xticklabels=True,\n","            yticklabels=True,\n","            cbar=True\n","        )\n","\n","        metric_names = {\n","            'silhouette': 'Silhouette Score',\n","            'davies_bouldin': 'Davies-Bouldin Index',\n","            'calinski_harabasz': 'Calinski-Harabasz Score'\n","        }\n","        title_suffix = f'(зеленый = значимые различия, p < {significance_level}'\n","        if n_nan_in_matrix > 0:\n","            title_suffix += f'; N/A = тест невозможен'\n","        title_suffix += ')'\n","        ax.set_title(f'{metric_names.get(metric, metric)}\\n{title_suffix}',\n","                    fontsize=12, fontweight='bold', pad=10)\n","        ax.set_xlabel('Метод 1', fontsize=10)\n","        ax.set_ylabel('Метод 2', fontsize=10)\n","\n","        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=9)\n","        ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=9)\n","\n","    plt.suptitle('Тепловые карты p-values между методами (статистическая значимость различий)',\n","                 fontsize=14, fontweight='bold', y=1.02)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return fig\n"]},{"cell_type":"markdown","metadata":{"id":"mcB40ss8koJx"},"source":["## Анализ чувствительности к параметрам\n","\n","Для понимания устойчивости алгоритма важно анализировать влияние гиперпараметров на результаты.\n","\n","### Анализ чувствительности к весам метрик\n","\n","Веса `weight_distance` и `weight_entropy` в комбинированной boundary метрике влияют на:\n","- Количество обнаруженных boundary точек\n","- Качество boundary кластеров (плотность, выравненность)\n","- Общее качество кластеризации (silhouette score)\n","\n","**Методология анализа**:\n","- Тестирование сетки значений весов (от 0.1 до 0.9)\n","- Вычисление метрик для каждой комбинации\n","- Визуализация зависимости метрик от весов\n","- Определение оптимальной комбинации весов\n","\n","Это позволяет:\n","1. Понять, какие веса дают лучшие результаты\n","2. Оценить устойчивость алгоритма к изменению весов\n","3. Найти баланс между метрикой расстояний и энтропии\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TynqFhUokoJ2","executionInfo":{"status":"ok","timestamp":1766063394422,"user_tz":-180,"elapsed":41,"user":{"displayName":"Ульяна","userId":"09654476276403892729"}},"outputId":"9b84d20c-841a-4184-e295-cc879ba89fa6","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Функции анализа чувствительности к весам определены\n"]}],"source":["# @title\n","def analyze_weight_sensitivity(latent_vectors, cluster_labels, X_original=None,\n","                               weight_combinations=None, boundary_threshold=0.5,\n","                               metrics_to_evaluate=None):\n","    \"\"\"\n","    Анализирует чувствительность boundary detection к весам метрик.\n","\n","    Параметры:\n","    -----------\n","    latent_vectors : np.ndarray\n","        Латентные представления данных\n","    cluster_labels : np.ndarray\n","        Метки основных кластеров\n","    X_original : np.ndarray, optional\n","        Исходные данные (используется только для анализа характеристик кластеров, если нужен)\n","        Метрики кластеризации вычисляются на латентном пространстве (latent_vectors)\n","    weight_combinations : list, optional\n","        Список кортежей (weight_distance, weight_entropy).\n","        Если None, используется сетка значений от 0.1 до 0.9 с шагом 0.1\n","    boundary_threshold : float\n","        Порог для определения boundary точек\n","    metrics_to_evaluate : list, optional\n","        Список метрик для оценки. Если None, используются все метрики.\n","\n","    Возвращает:\n","    -----------\n","    results_df : pd.DataFrame\n","        DataFrame с результатами для каждой комбинации весов\n","    \"\"\"\n","    if weight_combinations is None:\n","        # Создаем сетку весов: weight_distance от 0.1 до 0.9, weight_entropy = 1 - weight_distance\n","        weight_combinations = [(w, 1.0 - w) for w in np.arange(0.1, 1.0, 0.1)]\n","\n","    if metrics_to_evaluate is None:\n","        metrics_to_evaluate = ['silhouette', 'davies_bouldin', 'calinski_harabasz',\n","                              'boundary_ratio', 'boundary_density', 'boundary_alignment', 'boundary_stability']\n","\n","    results = []\n","\n","    print(\"=\"*80)\n","    print(\"АНАЛИЗ ЧУВСТВИТЕЛЬНОСТИ К ВЕСАМ МЕТРИК\")\n","    print(\"=\"*80)\n","    print(f\"Тестируем {len(weight_combinations)} комбинаций весов...\")\n","\n","    for idx, (w_dist, w_ent) in enumerate(weight_combinations):\n","        if idx % 5 == 0 or idx == len(weight_combinations) - 1:\n","            print(f\"  Комбинация {idx+1}/{len(weight_combinations)}: weight_distance={w_dist:.2f}, weight_entropy={w_ent:.2f}\")\n","\n","        # Обнаружение boundary кластеров с текущими весами\n","        boundary_mask, boundary_labels = detect_boundary_clusters(\n","            latent_vectors, cluster_labels,\n","            boundary_threshold=boundary_threshold,\n","            weight_distance=w_dist,\n","            weight_entropy=w_ent\n","        )\n","\n","        # Метрики вычисляются на латентном пространстве (latent_vectors)\n","        metrics = evaluate_clustering_metrics(latent_vectors, cluster_labels, boundary_labels)\n","\n","        # Сохраняем результаты\n","        result = {\n","            'weight_distance': w_dist,\n","            'weight_entropy': w_ent,\n","            'boundary_points': boundary_mask.sum(),\n","            'boundary_ratio': boundary_mask.sum() / len(cluster_labels) * 100\n","        }\n","\n","        # Добавляем метрики\n","        for metric in metrics_to_evaluate:\n","            if metric in metrics:\n","                result[metric] = metrics[metric]\n","            else:\n","                result[metric] = np.nan\n","\n","        results.append(result)\n","\n","    results_df = pd.DataFrame(results)\n","\n","    print(\"\\nАнализ завершен!\")\n","    print(f\"Лучшая комбинация по silhouette score:\")\n","    best_idx = results_df['silhouette'].idxmax()\n","    best_row = results_df.loc[best_idx]\n","    print(f\"   weight_distance={best_row['weight_distance']:.2f}, weight_entropy={best_row['weight_entropy']:.2f}\")\n","    print(f\"   silhouette={best_row['silhouette']:.4f}, boundary_ratio={best_row['boundary_ratio']:.2f}%\")\n","\n","    return results_df\n","\n","\n","def visualize_weight_sensitivity(results_df, figsize=(16, 10)):\n","    \"\"\"\n","    Визуализирует результаты анализа чувствительности к весам.\n","\n","    Параметры:\n","    -----------\n","    results_df : pd.DataFrame\n","        Результаты из analyze_weight_sensitivity\n","    figsize : tuple\n","        Размер фигуры\n","    \"\"\"\n","    fig, axes = plt.subplots(2, 3, figsize=figsize)\n","    axes = axes.flatten()\n","\n","    # 1. Silhouette Score\n","    ax = axes[0]\n","    ax.plot(results_df['weight_distance'], results_df['silhouette'], 'o-', linewidth=2, markersize=6)\n","    ax.set_xlabel('Weight Distance', fontsize=12)\n","    ax.set_ylabel('Silhouette Score', fontsize=12)\n","    ax.set_title('Silhouette Score vs Weight Distance', fontsize=14, fontweight='bold')\n","    ax.grid(True, alpha=0.3)\n","\n","    # 2. Davies-Bouldin Index\n","    ax = axes[1]\n","    ax.plot(results_df['weight_distance'], results_df['davies_bouldin'], 'o-', linewidth=2, markersize=6, color='orange')\n","    ax.set_xlabel('Weight Distance', fontsize=12)\n","    ax.set_ylabel('Davies-Bouldin Index', fontsize=12)\n","    ax.set_title('Davies-Bouldin Index vs Weight Distance\\n(чем ниже, тем лучше)', fontsize=14, fontweight='bold')\n","    ax.grid(True, alpha=0.3)\n","\n","    # 3. Boundary Ratio\n","    ax = axes[2]\n","    ax.plot(results_df['weight_distance'], results_df['boundary_ratio'], 'o-', linewidth=2, markersize=6, color='green')\n","    ax.set_xlabel('Weight Distance', fontsize=12)\n","    ax.set_ylabel('Boundary Ratio (%)', fontsize=12)\n","    ax.set_title('Boundary Ratio vs Weight Distance', fontsize=14, fontweight='bold')\n","    ax.grid(True, alpha=0.3)\n","\n","    # 4. Boundary Density\n","    ax = axes[3]\n","    ax.plot(results_df['weight_distance'], results_df['boundary_density'], 'o-', linewidth=2, markersize=6, color='purple')\n","    ax.set_xlabel('Weight Distance', fontsize=12)\n","    ax.set_ylabel('Boundary Density', fontsize=12)\n","    ax.set_title('Boundary Density vs Weight Distance', fontsize=14, fontweight='bold')\n","    ax.grid(True, alpha=0.3)\n","\n","    # 5. Boundary Alignment\n","    ax = axes[4]\n","    ax.plot(results_df['weight_distance'], results_df['boundary_alignment'], 'o-', linewidth=2, markersize=6, color='brown')\n","    ax.set_xlabel('Weight Distance', fontsize=12)\n","    ax.set_ylabel('Boundary Alignment', fontsize=12)\n","    ax.set_title('Boundary Alignment vs Weight Distance', fontsize=14, fontweight='bold')\n","    ax.grid(True, alpha=0.3)\n","\n","    # 6. Heatmap: Silhouette vs Weights\n","    ax = axes[5]\n","    # Создаем сетку для heatmap\n","    weight_distances = results_df['weight_distance'].values\n","    silhouette_scores = results_df['silhouette'].values\n","\n","\n","    from scipy.interpolate import interp1d\n","    if len(weight_distances) > 1:\n","        f = interp1d(weight_distances, silhouette_scores, kind='linear', fill_value='extrapolate')\n","        weight_distances_smooth = np.linspace(weight_distances.min(), weight_distances.max(), 50)\n","        silhouette_smooth = f(weight_distances_smooth)\n","\n","        ax.plot(weight_distances, silhouette_scores, 'o-', linewidth=2, markersize=8, label='Actual')\n","        ax.fill_between(weight_distances_smooth, silhouette_smooth, alpha=0.3, label='Interpolated')\n","        ax.set_xlabel('Weight Distance', fontsize=12)\n","        ax.set_ylabel('Silhouette Score', fontsize=12)\n","        ax.set_title('Silhouette Score: Detailed View', fontsize=14, fontweight='bold')\n","        ax.grid(True, alpha=0.3)\n","        ax.legend()\n","    else:\n","        ax.text(0.5, 0.5, 'Недостаточно данных\\nдля визуализации',\n","               ha='center', va='center', fontsize=12, transform=ax.transAxes)\n","\n","    plt.suptitle('Анализ чувствительности к весам метрик', fontsize=16, fontweight='bold', y=0.995)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return fig\n","\n","print(\"Функции анализа чувствительности к весам определены\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gngMk6f7koJ2","executionInfo":{"status":"ok","timestamp":1766063394501,"user_tz":-180,"elapsed":78,"user":{"displayName":"Ульяна","userId":"09654476276403892729"}},"outputId":"418a77f7-d6b4-4727-8bd8-d50f3e2ec915","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Функция визуализации сравнения fuzzy методов определена\n"]}],"source":["# @title\n","def visualize_fuzzy_comparison(fuzzy_comparison_df, figsize=(18, 5)):\n","    \"\"\"\n","    Визуализирует сравнение Fuzzy C-Means, Soft K-Means и VAE + KMeans + Boundary методов.\n","\n","    Параметры:\n","    -----------\n","    fuzzy_comparison_df : pd.DataFrame\n","        DataFrame с метриками всех методов (из compare_with_fuzzy_methods)\n","    figsize : tuple\n","        Размер фигуры\n","\n","    Возвращает:\n","    -----------\n","    fig : matplotlib.figure.Figure\n","        Объект фигуры\n","    \"\"\"\n","    if fuzzy_comparison_df is None or fuzzy_comparison_df.empty:\n","        print(\"Внимание: данные для сравнения fuzzy методов недоступны\")\n","        return None\n","\n","    # Метрики для сравнения\n","    metrics_to_plot = ['silhouette', 'davies_bouldin', 'calinski_harabasz', 'boundary_ratio']\n","\n","    # Проверяем, какие метрики доступны\n","    available_metrics = [m for m in metrics_to_plot if m in fuzzy_comparison_df.columns]\n","\n","    if len(available_metrics) == 0:\n","        print(\"Внимание: нет доступных метрик для визуализации\")\n","        return None\n","\n","\n","    n_metrics = len(available_metrics)\n","    fig, axes = plt.subplots(1, n_metrics, figsize=figsize)\n","\n","    if n_metrics == 1:\n","        axes = [axes]\n","\n","    methods = fuzzy_comparison_df['method'].values\n","\n","    colors_map = {\n","        'Fuzzy C-Means': 'steelblue',\n","        'Soft K-Means': 'orange',\n","        'VAE + KMeans + Boundary': 'green'\n","    }\n","\n","    # 1. Silhouette Score\n","    if 'silhouette' in available_metrics:\n","        idx = available_metrics.index('silhouette')\n","        ax = axes[idx]\n","        values = fuzzy_comparison_df['silhouette'].values\n","        colors = [colors_map.get(m, 'gray') for m in methods]\n","        bars = ax.barh(methods, values, color=colors, alpha=0.7)\n","        ax.set_xlabel('Silhouette Score', fontsize=12)\n","        ax.set_title('Silhouette Score\\n(чем выше, тем лучше)', fontsize=12, fontweight='bold')\n","        ax.grid(True, alpha=0.3, axis='x')\n","        # Добавляем значения на столбцы\n","        for i, (bar, val) in enumerate(zip(bars, values)):\n","            ax.text(val + 0.01, i, f'{val:.3f}', va='center', fontsize=9)\n","\n","    # 2. Davies-Bouldin Index\n","    if 'davies_bouldin' in available_metrics:\n","        idx = available_metrics.index('davies_bouldin')\n","        ax = axes[idx]\n","        values = fuzzy_comparison_df['davies_bouldin'].values\n","        colors = [colors_map.get(m, 'gray') for m in methods]\n","        bars = ax.barh(methods, values, color=colors, alpha=0.7)\n","        ax.set_xlabel('Davies-Bouldin Index', fontsize=12)\n","        ax.set_title('Davies-Bouldin Index\\n(чем ниже, тем лучше)', fontsize=12, fontweight='bold')\n","        ax.grid(True, alpha=0.3, axis='x')\n","        # Добавляем значения на столбцы\n","        for i, (bar, val) in enumerate(zip(bars, values)):\n","            ax.text(val + 0.05, i, f'{val:.3f}', va='center', fontsize=9)\n","\n","    # 3. Calinski-Harabasz Score\n","    if 'calinski_harabasz' in available_metrics:\n","        idx = available_metrics.index('calinski_harabasz')\n","        ax = axes[idx]\n","        values = fuzzy_comparison_df['calinski_harabasz'].values\n","        colors = [colors_map.get(m, 'gray') for m in methods]\n","        bars = ax.barh(methods, values, color=colors, alpha=0.7)\n","        ax.set_xlabel('Calinski-Harabasz Score', fontsize=12)\n","        ax.set_title('Calinski-Harabasz Score\\n(чем выше, тем лучше)', fontsize=12, fontweight='bold')\n","        ax.grid(True, alpha=0.3, axis='x')\n","\n","        for i, (bar, val) in enumerate(zip(bars, values)):\n","            if val > 1000:\n","                label = f'{val/1000:.1f}K'\n","            else:\n","                label = f'{val:.0f}'\n","            ax.text(val + val*0.02, i, label, va='center', fontsize=9)\n","\n","    # 4. Boundary Ratio\n","    if 'boundary_ratio' in available_metrics:\n","        idx = available_metrics.index('boundary_ratio')\n","        ax = axes[idx]\n","        values = fuzzy_comparison_df['boundary_ratio'].values\n","        colors = [colors_map.get(m, 'gray') for m in methods]\n","        bars = ax.barh(methods, values, color=colors, alpha=0.7)\n","        ax.set_xlabel('Boundary Ratio (%)', fontsize=12)\n","        ax.set_title('Boundary Ratio\\n(целевой диапазон 5-15%)', fontsize=12, fontweight='bold')\n","        ax.grid(True, alpha=0.3, axis='x')\n","        # Добавляем значения на столбцы\n","        for i, (bar, val) in enumerate(zip(bars, values)):\n","            ax.text(val + 0.5, i, f'{val:.2f}%', va='center', fontsize=9)\n","\n","    plt.suptitle('Сравнение Fuzzy методов с VAE + KMeans + Boundary',\n","                 fontsize=14, fontweight='bold', y=1.02)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return fig\n","\n","print(\"Функция визуализации сравнения fuzzy методов определена\")\n"]},{"cell_type":"markdown","metadata":{"id":"fsydsfXrkoJx"},"source":["## Сравнение с методами мягкой кластеризации\n","\n","Мягкая кластеризация также позволяет работать с неопределенностью принадлежности точек к кластерам, что концептуально близко к нашей идее boundary кластеров.\n","\n","### Fuzzy C-Means\n","\n","**Принцип**: каждая точка имеет степень принадлежности к каждому кластеру, а не жесткую метку.\n","\n","**Boundary точки в Fuzzy C-Means**:\n","- Определяются через энтропию принадлежностей\n","- Высокая энтропия = неопределенность = boundary точка\n","- Группируются с помощью DBSCAN\n","\n","### Soft K-Means\n","\n","**Принцип**: итеративное обновление центров кластеров с учетом мягких принадлежностей точек.\n","\n","**Отличия от нашего метода**:\n","- Fuzzy методы работают на исходном пространстве признаков\n","- Наш метод использует латентное пространство VAE для лучшей разделимости\n","- Наш метод явно группирует boundary точки в отдельные кластеры\n","\n","### Методологическое сравнение\n","\n","Сравнение показывает:\n","- Преимущества использования латентного пространства VAE\n","- Эффективность интегральной boundary метрики\n","- Практическую применимость boundary кластеров для бизнес-задач\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"igppP5KmkoJ2","executionInfo":{"status":"ok","timestamp":1766063394550,"user_tz":-180,"elapsed":48,"user":{"displayName":"Ульяна","userId":"09654476276403892729"}},"outputId":"a39de7bc-74ca-4743-ab22-a6e0fee3618a","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Функция сравнения с fuzzy методами определена\n"]}],"source":["# @title\n","try:\n","    from sklearn_extensions.fuzzy_kmeans import FuzzyKMeans\n","    FUZZY_KMEANS_AVAILABLE = True\n","except ImportError:\n","    try:\n","        import skfuzzy as fuzz\n","        FUZZY_C_MEANS_AVAILABLE = True\n","        FUZZY_KMEANS_AVAILABLE = False\n","    except ImportError:\n","        FUZZY_C_MEANS_AVAILABLE = False\n","        FUZZY_KMEANS_AVAILABLE = False\n","        print(\"Библиотеки для fuzzy clustering недоступны.\")\n","        print(\"   Для установки: pip install scikit-fuzzy\")\n","        print(\"   Или: pip install sklearn-extensions\")\n","\n","try:\n","    from sklearn.cluster import SpectralClustering\n","    SPECTRAL_AVAILABLE = True\n","except ImportError:\n","    SPECTRAL_AVAILABLE = False\n","\n","def compare_with_fuzzy_methods(X, results, n_clusters=None, random_state=None):\n","    \"\"\"\n","    Сравнивает наш метод с методами мягкой кластеризации (fuzzy C-means, soft K-means).\n","\n","    Параметры:\n","    -----------\n","    X : np.ndarray\n","        Предобработанные данные\n","    results : dict\n","        Результаты VAE + KMeans + Boundary метода\n","    n_clusters : int, optional\n","        Количество кластеров. Если None, берется из results\n","    random_state : int, optional\n","        Random state для воспроизводимости\n","\n","    Возвращает:\n","    -----------\n","    fuzzy_results : dict\n","        Словарь с результатами fuzzy методов\n","    comparison_df : pd.DataFrame\n","        DataFrame с метриками всех методов\n","    \"\"\"\n","    if random_state is None:\n","        random_state = RANDOM_STATE\n","\n","    if n_clusters is None:\n","        n_clusters = len(np.unique(results['cluster_labels']))\n","\n","    fuzzy_results = {}\n","    comparison_data = []\n","\n","    print(\"=\"*80)\n","    print(\"СРАВНЕНИЕ С FUZZY МЕТОДАМИ\")\n","    print(\"=\"*80)\n","\n","    # 1. Fuzzy C-Means\n","    if FUZZY_C_MEANS_AVAILABLE:\n","        try:\n","            print(\"\\n1. Fuzzy C-Means (scikit-fuzzy)...\")\n","            start_time = time.time()\n","\n","            # Транспонируем данные для scikit-fuzzy\n","            X_fuzzy = X.T\n","\n","            # Инициализация центров кластеров\n","            cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(\n","                X_fuzzy, n_clusters, 2, error=0.005, maxiter=1000, init=None, seed=random_state\n","            )\n","\n","            # Получаем метки кластеров (кластер с максимальной принадлежностью)\n","            fuzzy_labels = np.argmax(u, axis=0)\n","\n","            membership_entropy = entropy(u, axis=0)\n","            # Нормализуем энтропию\n","            membership_entropy_norm = (membership_entropy - membership_entropy.min()) / (membership_entropy.max() - membership_entropy.min() + 1e-10)\n","\n","            # Boundary точки: высокое значение энтропии принадлежностей\n","            boundary_threshold_fuzzy = 0.5\n","            fuzzy_boundary_mask = membership_entropy_norm >= boundary_threshold_fuzzy\n","\n","            # Создаем boundary labels (используем DBSCAN для группировки boundary точек)\n","            fuzzy_boundary_labels = np.full(len(fuzzy_labels), -1)\n","            if fuzzy_boundary_mask.sum() > 0:\n","                boundary_vectors_fuzzy = X[fuzzy_boundary_mask]\n","                if len(boundary_vectors_fuzzy) > 1:\n","                    # Используем DBSCAN для группировки boundary точек\n","                    nbrs_fuzzy = NearestNeighbors(n_neighbors=min(10, len(boundary_vectors_fuzzy))).fit(boundary_vectors_fuzzy)\n","                    distances_fuzzy, _ = nbrs_fuzzy.kneighbors(boundary_vectors_fuzzy)\n","                    k_distances_fuzzy = np.sort(distances_fuzzy[:, -1])\n","                    eps_fuzzy = np.percentile(k_distances_fuzzy, 90)\n","                    min_samples_fuzzy = max(5, len(boundary_vectors_fuzzy) // 20)\n","\n","                    dbscan_fuzzy = DBSCAN(eps=eps_fuzzy, min_samples=min_samples_fuzzy)\n","                    boundary_cluster_labels_fuzzy = dbscan_fuzzy.fit_predict(boundary_vectors_fuzzy)\n","\n","                    boundary_idx = 0\n","                    for i in range(len(fuzzy_labels)):\n","                        if fuzzy_boundary_mask[i]:\n","                            if boundary_cluster_labels_fuzzy[boundary_idx] != -1:\n","                                fuzzy_boundary_labels[i] = boundary_cluster_labels_fuzzy[boundary_idx] + n_clusters\n","                            boundary_idx += 1\n","\n","            fuzzy_time = time.time() - start_time\n","\n","            # Вычисляем метрики\n","            # Метрики вычисляются на исходном пространстве (X)\n","            fuzzy_metrics = evaluate_clustering_metrics(X, fuzzy_labels, fuzzy_boundary_labels)\n","            fuzzy_metrics['method'] = 'Fuzzy C-Means'\n","            fuzzy_metrics['execution_time'] = fuzzy_time\n","            fuzzy_metrics['fpc'] = fpc  # Fuzzy partition coefficient\n","\n","            fuzzy_results['Fuzzy C-Means'] = {\n","                'labels': fuzzy_labels,\n","                'boundary_labels': fuzzy_boundary_labels,\n","                'membership_matrix': u,\n","                'metrics': fuzzy_metrics\n","            }\n","\n","            comparison_data.append(fuzzy_metrics)\n","            print(f\"   Время выполнения: {fuzzy_time:.2f} сек\")\n","            print(f\"   Fuzzy Partition Coefficient (FPC): {fpc:.4f} (чем выше, тем лучше)\")\n","            print(f\"   Silhouette Score: {fuzzy_metrics['silhouette']:.4f}\")\n","            print(f\"   Boundary точек: {fuzzy_boundary_mask.sum()} ({fuzzy_boundary_mask.sum()/len(fuzzy_labels)*100:.2f}%)\")\n","\n","        except Exception as e:\n","            print(f\"   Ошибка при выполнении Fuzzy C-Means: {e}\")\n","\n","    # 2. Soft K-Means (используя sklearn-extensions или собственную реализацию)\n","    print(\"\\n2. Soft K-Means (собственная реализация)...\")\n","    try:\n","        start_time = time.time()\n","\n","        # Инициализация центров (используем KMeans)\n","        kmeans_init = KMeans(n_clusters=n_clusters, n_init=1, random_state=random_state)\n","        kmeans_init.fit(X)\n","        centers = kmeans_init.cluster_centers_\n","\n","        # Soft K-Means: итеративное обновление центров с мягкими принадлежностями\n","        # Используем экспоненциальное взвешивание для мягких меток\n","        beta = 1.0  # Параметр температуры (чем больше, тем \"жестче\" кластеризация)\n","        max_iter = 100\n","        tolerance = 1e-4\n","\n","        for iteration in range(max_iter):\n","            # Вычисляем расстояния до всех центров\n","            distances = cdist(X, centers)\n","\n","            # Мягкие принадлежности: exp(-beta * distance) / sum(exp(-beta * distance))\n","            exp_distances = np.exp(-beta * distances)\n","            soft_memberships = exp_distances / exp_distances.sum(axis=1, keepdims=True)\n","\n","            # Обновляем центры как взвешенное среднее\n","            new_centers = (soft_memberships.T @ X) / soft_memberships.sum(axis=0, keepdims=True).T\n","\n","            # Проверяем сходимость\n","            if np.linalg.norm(new_centers - centers) < tolerance:\n","                break\n","            centers = new_centers\n","\n","        # Получаем жесткие метки (кластер с максимальной принадлежностью)\n","        soft_labels = np.argmax(soft_memberships, axis=1)\n","\n","        # Boundary точки: точки с высокой энтропией принадлежностей\n","        membership_entropy_soft = entropy(soft_memberships, axis=1)\n","        membership_entropy_soft_norm = (membership_entropy_soft - membership_entropy_soft.min()) / (membership_entropy_soft.max() - membership_entropy_soft.min() + 1e-10)\n","\n","        boundary_threshold_soft = 0.5\n","        soft_boundary_mask = membership_entropy_soft_norm >= boundary_threshold_soft\n","\n","        # Создаем boundary labels\n","        soft_boundary_labels = np.full(len(soft_labels), -1)\n","        if soft_boundary_mask.sum() > 0:\n","            boundary_vectors_soft = X[soft_boundary_mask]\n","            if len(boundary_vectors_soft) > 1:\n","                nbrs_soft = NearestNeighbors(n_neighbors=min(10, len(boundary_vectors_soft))).fit(boundary_vectors_soft)\n","                distances_soft, _ = nbrs_soft.kneighbors(boundary_vectors_soft)\n","                k_distances_soft = np.sort(distances_soft[:, -1])\n","                eps_soft = np.percentile(k_distances_soft, 90)\n","                min_samples_soft = max(5, len(boundary_vectors_soft) // 20)\n","\n","                dbscan_soft = DBSCAN(eps=eps_soft, min_samples=min_samples_soft)\n","                boundary_cluster_labels_soft = dbscan_soft.fit_predict(boundary_vectors_soft)\n","\n","                boundary_idx = 0\n","                for i in range(len(soft_labels)):\n","                    if soft_boundary_mask[i]:\n","                        if boundary_cluster_labels_soft[boundary_idx] != -1:\n","                            soft_boundary_labels[i] = boundary_cluster_labels_soft[boundary_idx] + n_clusters\n","                        boundary_idx += 1\n","\n","        soft_time = time.time() - start_time\n","\n","        # Вычисляем метрики\n","        # Метрики вычисляются на исходном пространстве (X)\n","        soft_metrics = evaluate_clustering_metrics(X, soft_labels, soft_boundary_labels)\n","        soft_metrics['method'] = 'Soft K-Means'\n","        soft_metrics['execution_time'] = soft_time\n","        soft_metrics['avg_entropy'] = membership_entropy_soft.mean()  # Средняя энтропия принадлежностей\n","\n","        fuzzy_results['Soft K-Means'] = {\n","            'labels': soft_labels,\n","            'boundary_labels': soft_boundary_labels,\n","            'membership_matrix': soft_memberships,\n","            'metrics': soft_metrics\n","        }\n","\n","        comparison_data.append(soft_metrics)\n","        print(f\"   Время выполнения: {soft_time:.2f} сек\")\n","        print(f\"   Средняя энтропия принадлежностей: {soft_metrics['avg_entropy']:.4f}\")\n","        print(f\"   Silhouette Score: {soft_metrics['silhouette']:.4f}\")\n","        print(f\"   Boundary точек: {soft_boundary_mask.sum()} ({soft_boundary_mask.sum()/len(soft_labels)*100:.2f}%)\")\n","\n","    except Exception as e:\n","        print(f\"   Ошибка при выполнении Soft K-Means: {e}\")\n","\n","    # 3. Наш метод (VAE + KMeans + Boundary)\n","    vae_metrics = results['metrics'].copy()\n","    vae_metrics['method'] = 'VAE + KMeans + Boundary'\n","    if 'execution_time' in results:\n","        vae_metrics['execution_time'] = results['execution_time']\n","    comparison_data.append(vae_metrics)\n","\n","    comparison_df = pd.DataFrame(comparison_data)\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"ИТОГОВОЕ СРАВНЕНИЕ\")\n","    print(\"=\"*80)\n","    print(comparison_df[['method', 'silhouette', 'davies_bouldin', 'calinski_harabasz',\n","                         'boundary_ratio', 'execution_time']].to_string(index=False))\n","\n","    return fuzzy_results, comparison_df\n","\n","print(\"Функция сравнения с fuzzy методами определена\")\n"]},{"cell_type":"markdown","metadata":{"id":"xTQvT1Bq0_Z7"},"source":["## Запуск\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1v-Y8r-qDZcw69UPmA6JW24XgmDsgcCM-"},"id":"NR91Kz9q0_Z7","outputId":"0cb008e0-7ba1-4bc9-a2a1-b46b2ddcc2fd","executionInfo":{"status":"ok","timestamp":1766069074735,"user_tz":-180,"elapsed":319747,"user":{"displayName":"Ульяна","userId":"09654476276403892729"}},"cellView":"form"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# @title\n","df = pd.read_csv('online_shoppers_intention.csv')\n","\n","results = run_vae_boundary_clustering(\n","    df,\n","    exclude_columns=None,  # Столбцы для исключения (например, ID)\n","    latent_dim=20,  # Размерность латентного пространства\n","    hidden_dims=[512, 256, 128, 64],  # Архитектура скрытых слоев VAE\n","    n_clusters=4,  # Количество основных кластеров (оптимизируется через optimize=True)\n","    boundary_threshold=0.5,  # Порог для определения boundary точек (0-1)\n","    max_boundary_clusters=None,  # Максимальное количество boundary кластеров (None = ограничено n_clusters)\n","    vae_epochs=250,  # Количество эпох обучения VAE\n","    vae_beta=2.5,  # Коэффициент β для β-VAE (было 4.0)\n","    vae_batch_size=256,  # Размер батча для обучения VAE\n","    vae_lr=5e-4,  # Learning rate для обучения VAE\n","    vae_dropout=0.2,  # Вероятность dropout для VAE\n","    use_batch_norm=True,  # Использовать ли Batch Normalization\n","    early_stopping_patience=30,  # Количество эпох без улучшения\n","    optimize=True,  # Включить Bayesian Optimization\n","    max_optimization_iterations=15,  # Количество итераций оптимизации\n","    n_clusters_range=(3, 6),  # Диапазон количества кластеров для оптимизации (min, max)\n","    # Если None, используется диапазон вокруг n_clusters: (max(2, n_clusters-2), min(n_clusters+3, 15))\n","    verbose=True  # Выводить подробную информацию\n",")\n","\n","# ========== 1. ВИЗУАЛИЗАЦИЯ НАШЕЙ МОДЕЛИ ==========\n","print(\"\\n\" + \"=\"*80)\n","print(\"ВИЗУАЛИЗАЦИЯ НАШЕЙ МОДЕЛИ\")\n","print(\"=\"*80)\n","\n","# График истории обучения VAE\n","print(\"\\nГрафик истории обучения VAE:\")\n","plot_training_history(results)\n","\n","# 2D и 3D визуализация нашей модели\n","visualize_clusters(results, method_name=\"VAE + KMeans + Boundary\")\n","\n","# ========== 1.5. АНАЛИЗ ЧУВСТВИТЕЛЬНОСТИ К ВЕСАМ МЕТРИК ==========\n","\n","weight_sensitivity_results = analyze_weight_sensitivity(\n","    results['latent_vectors'],\n","    results['cluster_labels'],\n","    results['X_processed'],\n","    boundary_threshold=0.5\n",")\n","\n","# Визуализация результатов анализа чувствительности\n","visualize_weight_sensitivity(weight_sensitivity_results)\n","\n","# ========== 1.6. СРАВНЕНИЕ С FUZZY МЕТОДАМИ ==========\n","\n","# Сравнение с методами мягкой кластеризации\n","fuzzy_results, fuzzy_comparison_df = compare_with_fuzzy_methods(\n","    results['X_processed'],\n","    results,\n","    n_clusters=len(np.unique(results['cluster_labels']))\n",")\n","\n","# Визуализация сравнения fuzzy методов\n","if fuzzy_comparison_df is not None and not fuzzy_comparison_df.empty:\n","    print(\"\\nВизуализация сравнения Fuzzy методов:\")\n","    visualize_fuzzy_comparison(fuzzy_comparison_df)\n","\n","# ========== 2. СРАВНЕНИЕ С BASELINE МЕТОДАМИ ==========\n","# Сравнение методов\n","comparison_df, all_metrics_runs = compare_with_baselines(\n","    results['X_processed'],\n","    results,\n","    n_runs=10\n",")\n","\n","# Визуализация сравнения методов в 2D\n","visualize_methods_comparison_2d_no_boundary(results['X_processed'], results)\n","\n","# Визуализация сравнения методов в 3D\n","visualize_methods_comparison_3d(results['X_processed'], results)\n","\n","# Визуализация сравнения метрик\n","fig_metrics, _ = visualize_metrics_comparison(results['X_processed'], results, comparison_df)\n","\n","# ========== 3. СТАТИСТИЧЕСКОЕ СРАВНЕНИЕ ==========\n","\n","# Статистическое сравнение методов\n","pvalue_matrices, friedman_results = statistical_comparison(\n","    all_metrics_runs,\n","    metrics_to_compare=['silhouette', 'davies_bouldin', 'calinski_harabasz'],\n","    correction_method='bonferroni'  # Можно использовать 'fdr_bh' для менее консервативной коррекции\n",")\n","\n","# Тепловые карты p-values\n","plot_pvalue_heatmap(\n","    pvalue_matrices,\n","    significance_level=0.05\n",")\n","\n","# ========== 4. ОПИСАНИЕ КЛАСТЕРОВ ==========\n","\n","# Вывод описания кластеров\n","print_cluster_descriptions(\n","    results['cluster_descriptions'],\n","    results['cluster_statistics'],\n","    verbose=True\n",")\n","\n","# ========== 5. ИТОГОВЫЕ РЕЗУЛЬТАТЫ ==========\n","print(\"\\n\" + \"=\"*80)\n","print(\"ИТОГОВЫЕ РЕЗУЛЬТАТЫ ИССЛЕДОВАНИЯ\")\n","print(\"=\"*80)\n","\n","print(f\"\\nОсновные результаты:\")\n","print(f\"   - Найдено основных кластеров: {len(np.unique(results['cluster_labels']))}\")\n","print(f\"   - Найдено boundary кластеров: {len(np.unique(results['boundary_labels'][results['boundary_labels'] != -1]))}\")\n","print(f\"   - Boundary точек: {(results['boundary_labels'] != -1).sum()} ({(results['boundary_labels'] != -1).sum()/len(df)*100:.2f}%)\")\n","\n","print(f\"\\nМетрики кластеризации:\")\n","for metric, value in results['metrics'].items():\n","    if not np.isnan(value) and not np.isinf(value):\n","        print(f\"   - {metric:25s}: {value:.4f}\")\n","\n","print(f\"\\nСравнение с базовыми методами:\")\n","print(comparison_df[['method', 'silhouette', 'davies_bouldin', 'calinski_harabasz']].to_string(index=False))\n","\n","# Добавляем информацию о fuzzy методах\n","if 'fuzzy_comparison_df' in locals() and fuzzy_comparison_df is not None:\n","    print(f\"\\nСравнение с fuzzy методами:\")\n","    print(fuzzy_comparison_df[['method', 'silhouette', 'davies_bouldin', 'calinski_harabasz', 'boundary_ratio']].to_string(index=False))\n","\n","# Добавляем информацию о лучших весах из анализа чувствительности\n","if 'weight_sensitivity_results' in locals() and weight_sensitivity_results is not None:\n","    best_weight_idx = weight_sensitivity_results['silhouette'].idxmax()\n","    best_weight_row = weight_sensitivity_results.loc[best_weight_idx]\n","    print(f\"\\nАнализ чувствительности к весам:\")\n","    print(f\"   Лучшая комбинация весов:\")\n","    print(f\"   - weight_distance: {best_weight_row['weight_distance']:.2f}\")\n","    print(f\"   - weight_entropy: {best_weight_row['weight_entropy']:.2f}\")\n","    print(f\"   - Silhouette Score: {best_weight_row['silhouette']:.4f}\")\n","    print(f\"   - Boundary Ratio: {best_weight_row['boundary_ratio']:.2f}%\")\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}